{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import operator as op\n",
    "import docopt\n",
    "import numpy as np\n",
    "import os.path\n",
    "import itertools\n",
    "from os import path\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import jensenshannon, cosine\n",
    "from numpy import asarray\n",
    "import statistics \n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics.pairwise import cosine_similarity, chi2_kernel\n",
    "from scipy.spatial import distance\n",
    "import joblib\n",
    "import shutil\n",
    "\n",
    "Report = namedtuple(\"Report\", [\"precision\", \"recall\", \"accuracy\", \"f1\", \"tp\", \"tn\", \"fp\", \"fn\"])\n",
    "JSON = \"unified-dataset.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "(4868, 5000)\n",
      "(4868, 5000)\n",
      "(4868, 5000)\n",
      "(7666, 5000)\n",
      "(4868, 5000)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:1279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  q = q / np.sum(q, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65300289 0.71217394 0.65470119 ...        nan 0.83255461        nan]\n",
      "0.48101544698034715\n"
     ]
    }
   ],
   "source": [
    "#test code for understanding how NP arrays are distributed\n",
    "arr1 = np.load(\"ssec_emotion-causetrain_xNP.npy\")\n",
    "arr2 = np.load(\"ssec_grounded_emotionstrain_xNP.npy\")\n",
    "arr3 = np.load(\"isear_ssectest_xNP.npy\")\n",
    "arr4 = np.load(\"isear_ssectrain_xNP.npy\")\n",
    "arr5 = np.load(\"ssec_iseartest_xNP.npy\")\n",
    "arr5 = np.load(\"ssec_iseartrain_xNP.npy\")\n",
    "print(np.array_equal(arr1,arr2))\n",
    "print(np.array_equal(arr2,arr3))\n",
    "print(np.array_equal(arr3,arr4))\n",
    "print(np.array_equal(arr4,arr5))\n",
    "print(np.array_equal(arr1,arr5))\n",
    "print(np.array_equal(arr3,arr5))\n",
    "print(arr1.shape)\n",
    "print(arr2.shape)\n",
    "print(arr3.shape)\n",
    "print(arr4.shape)\n",
    "print(arr5.shape)\n",
    "print(getJensenShannonFromNPArrays(arr1,arr2))\n",
    "print(getJensenShannonFromNPArrays(arr2,arr3))\n",
    "# print(getJensenShannonFromNPArrays(arr4,arr5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method is used to get the classifier mode and decide whether to single-label or multi-label classification\n",
    "#this method comes from the original authors and is kept to replicate their results\n",
    "def get_clf_mode(train, test):\n",
    "    first = \"single\"\n",
    "    for example in train:\n",
    "        if example.get(\"labeled\", \"multi\") == \"multi\":\n",
    "            first = \"multi\"\n",
    "    print(first)\n",
    "    for example in test:\n",
    "        if example.get(\"labeled\", \"multi\") == \"multi\":\n",
    "            return first, \"multi\"\n",
    "    print(\"oof\")\n",
    "    return first, \"single\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This methods is used to extract the training and testing data from the unified corpus json.\n",
    "#The unified corpus json must be produced using the authors original code\n",
    "#this version is only used in getting the benchmarks for the previous paper\n",
    "#this version takes the jsonfile, the name of the train file and the name of the test file as parameters\n",
    "def get_train_test(jsonfile, train, test):\n",
    "    print(\"get_train_test param:\")\n",
    "    print(\"json \", jsonfile)\n",
    "    print(\"train \", train)\n",
    "    print(\"test \", test)\n",
    "    same = test in train.split(\",\") #used if train and test corpus are same\n",
    "    training, testing = [], []\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    with open(jsonfile) as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if(data[\"source\"] == test):\n",
    "                count1 += 1\n",
    "            if(data[\"source\"] != test):\n",
    "                count2 += 1\n",
    "            if(train == None and data[\"source\"] != test):\n",
    "                count3 += 1\n",
    "                training.append(data)\n",
    "            elif data[\"source\"] == test:\n",
    "                count4 += 1\n",
    "                testing.append(data)\n",
    "            elif(data[\"source\"] in train.split(\",\")):\n",
    "                count3 += 1\n",
    "                training.append(data)\n",
    "    print(\"there were \", count1, \" entries that were in test and \", count2, \"that were not in test\",\n",
    "          \"and \", count3, \" that were in train\")\n",
    "    print(\"test was appended \", count4, \" times\")\n",
    "    if same:\n",
    "        training, testing = hacky_train_test_split(testing, train_size=0.8, first=train, second=test)\n",
    "        print(\"revised\", \"there were \", len(testing), \" entries that were in test and \", len(training), \" that were in train\")\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method stays as is from the original paper\n",
    "def get_labels(train, test, operation=op.and_, mode=\"multi\"):\n",
    "    \"\"\"Return a list of the emotional intersection of two sources.\"\"\"\n",
    "    emotions = set()\n",
    "    if mode == \"single\":\n",
    "        emotions.add(\"noemo\")\n",
    "    train_emotions = set(\n",
    "        emotion\n",
    "        for data in train\n",
    "        for emotion in data[\"emotions\"]\n",
    "        if data[\"emotions\"][emotion] is not None\n",
    "    )\n",
    "    # print(train_emotions)\n",
    "    test_emotions = set(\n",
    "        emotion\n",
    "        for emotion in test[0][\"emotions\"]\n",
    "        if test[0][\"emotions\"][emotion] is not None\n",
    "    )\n",
    "    # print(test_emotions)\n",
    "    return list(emotions | operation(train_emotions, test_emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expects corpus list in data form\n",
    "#returns compatible labels\n",
    "def getMatchingLabels(corpora):\n",
    "    emotionSetList = []\n",
    "    for corpus in corpora:\n",
    "        emoSet = set(emotion for data in corpus for emotion in data[\"emotions\"] if data[\"emotions\"][emotion] is not None)\n",
    "        emotionSetList.append(emoSet)\n",
    "    intersectionSet = set.intersection(*emotionSetList)\n",
    "    print(intersectionSet)\n",
    "    return intersectionSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method stays as is from the original paper\n",
    "def get_emotion(emovals, labels, emotions, mode=\"multi\"):\n",
    "#     print(\"get emotion mode \", mode)\n",
    "#     print(\"emovals \",emovals)\n",
    "#     print(\"labels \",labels)\n",
    "#     print(\"emotions \",emotions)\n",
    "    if mode == \"single\":\n",
    "        truthy = len(list(filter(bool, emovals.values())))\n",
    "        if truthy == 1:\n",
    "            emotion = [v for v in emovals if emovals[v]][0]\n",
    "        elif truthy == 0:\n",
    "            emotion = \"noemo\"\n",
    "        else:\n",
    "            raise ValueError(\"Dataset marked as 'single' contains multiple emotions\")\n",
    "        return emotions.get(emotion, emotions.get(\"noemo\"))\n",
    "    else:\n",
    "        el = [int((emovals[label] or 0) > 0.1) for label in labels]\n",
    "        return np.array(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method stays as is from the original paper\n",
    "def get_vector(text, wordlist):\n",
    "    tokens = set(tokenize(text))\n",
    "#     print(tokens)\n",
    "    return [1 if word in tokens else 0 for word in wordlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The comment below was left by the original authors. As you can see, their results were unable to use the full bag of words\n",
    "# this is bad. memory error for all_vs (too many words...)\n",
    "def get_wordlist(dataset):\n",
    "    \"\"\"Get a bag of words from a dataset.\"\"\"\n",
    "    bag = set()\n",
    "    for data in dataset:\n",
    "        bag.update({token for token in tokenize(data[\"text\"])})\n",
    "    return list(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of word limit of 5000 is kept from the original authors to match their results\n",
    "def getTop5000Words(dataset):\n",
    "    \"\"\"Get a bag of words from a dataset.\"\"\"\n",
    "    bag = Counter()\n",
    "    for data in dataset:\n",
    "        bag.update({token for token in tokenize(data[\"text\"])})\n",
    "    print(\"bag size\", len(bag))\n",
    "#     print(\"bag\", bag)\n",
    "    out = list(map(op.itemgetter(0), bag.most_common(5000)))\n",
    "#     print(\"this is the output\", out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from my own Ling 413 final project, I was going to run trials with lemmatization and other tokenization\n",
    "#but by the time I was far enough in the project to do this, I didn't have time to run trials with this\n",
    "# def cleanDataLemma(dataset):\n",
    "#     taggedDataset = nltk.pos_tag(dataset)\n",
    "#     filteredString = []\n",
    "#     for token, tag in taggedDataset:\n",
    "#         for char in token:\n",
    "#             if char in string.punctuation:\n",
    "#                 token = token.replace(char,\"\") #remove punctuation\n",
    "#         if (token not in stopWords):\n",
    "#             lemmatizedToken = \"\"\n",
    "#             if tag[0] == 'N':\n",
    "#                 lemmatizedToken = lemmatizer.lemmatize(token, 'n')\n",
    "#             elif tag[0] == 'V':\n",
    "#                 lemmatizedToken = lemmatizer.lemmatize(token, 'v')\n",
    "#             else:\n",
    "#                 lemmatizedToken = token\n",
    "#             if len(lemmatizedToken) > 2:\n",
    "#                 filteredString.append(lemmatizedToken)\n",
    "#     return filteredString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization is kept the same so that performance results match the ones used in the paper as closely as possible\n",
    "#if there is improvement, it should be because of my changes\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\p{L}+\", text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCountsByEmotion(dataset, emotionLabels):\n",
    "    emotionCounts = []\n",
    "    print(\"emotions\")\n",
    "    for emotion in emotionLabels:\n",
    "        emotionDict = Counter()\n",
    "        for data in dataset:\n",
    "#             if data[\"emotions\"][emotion] == 1:\n",
    "#                 print(emotion)\n",
    "#                 print(data)\n",
    "#                 print(data[\"emotions\"][emotion])\n",
    "            emotionDict.update({token for token in tokenize(data[\"text\"]) if data[\"emotions\"][emotion] == 1})\n",
    "        print(len(emotionDict))\n",
    "        emotionCounts.append(emotionDict)\n",
    "    return emotionCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokenFrequency(dataset):\n",
    "    token2DocFreq = {}\n",
    "    for data in dataset:\n",
    "        tempDict = {}\n",
    "        for word in data:\n",
    "            if word not in tempDict:\n",
    "                tempDict[word] = 1\n",
    "        for key, value in tempDict.items():\n",
    "            if key in token2DocFreq:\n",
    "                token2DocFreq[key] += value\n",
    "            else:\n",
    "                token2DocFreq[key] = value\n",
    "    return token2DocFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokenizedCorpusTextPair(corpus1, corpus2):\n",
    "    with open(JSON) as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if data[\"source\"] in corporaNameList:\n",
    "                corpus1Text.append(tokenize(data[\"text\"]))\n",
    "                corpus1Data.append(data)\n",
    "            if data[\"source\"] in corporaNameList:\n",
    "                corpus2Text.append(tokenize(data[\"text\"]))\n",
    "                corpus2Data.append(data)\n",
    "    corporaData = [corpus1Data,corpus2Data]\n",
    "    return corpus1Text, corpus2Text, corporaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getNormalizedFreq(tokenFreq):\n",
    "# def getNormalizedFreq(corpus):\n",
    "#     newCorpus = []\n",
    "#     for entry in corpus:\n",
    "#         newCorpus.append(tokenize(entry))\n",
    "#     tokenFreq = getTokenFrequency(newCorpus)\n",
    "#     print(tokenFreq.items())\n",
    "#     print(\"freq values\", tokenFreq)\n",
    "    newTokenFreq = tokenFreq.copy()\n",
    "    for item, freq in newTokenFreq.items():\n",
    "        if(freq == 0):\n",
    "            newTokenFreq[item] = 0\n",
    "        else:\n",
    "            newTokenFreq[item] = 1 + math.log10(freq)\n",
    "#     print(\"log weighted values\", tokenFreq)\n",
    "    docLength = 0\n",
    "    for freq in newTokenFreq.values():\n",
    "        docLength += freq*freq\n",
    "    docLength = math.sqrt(docLength)\n",
    "#     print(\"doclength\", docLength)\n",
    "    for item, freq in newTokenFreq.items():\n",
    "        newTokenFreq[item] = freq/docLength\n",
    "    # logFreq = freq for freq in math.log() \n",
    "#     print(\"normalized\")\n",
    "#     print(tokenFreq)\n",
    "    return newTokenFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCosineSimilarityFromTokenFreq(tokenFreq1, tokenFreq2):\n",
    "    normFreq1 = getNormalizedFreq(tokenFreq1)\n",
    "    normFreq2 = getNormalizedFreq(tokenFreq2)\n",
    "    cosineSum = 0\n",
    "    normFreq1.items()\n",
    "    print(\"length 1\", len(normFreq1))\n",
    "    print(\"length 2\", len(normFreq2))\n",
    "    intersection = normFreq1.keys() & normFreq2.keys()\n",
    "    #only loop intersection because unshared values will be multiplied by 0 anyway\n",
    "    for item in intersection:\n",
    "#         if normFreq1[item] < 0 or normFreq2[item] < 0 :\n",
    "#             print(\"negative?\", item, normFreq1[item],normFreq2[item])\n",
    "#             sys.exit()\n",
    "#         print(item)\n",
    "#         print(normFreq1[item])\n",
    "#         print(normFreq2[item])\n",
    "        x = normFreq1[item] * normFreq2[item]\n",
    "        cosineSum += x\n",
    "    return cosineSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCosineSimilarityFromCorpus(corpus1,corpus2):\n",
    "    corpus1Text, corpus2Text, corpus1Data, corpus2Data = getTokenizedCorpusTextPair(corpus1, corpus2)\n",
    "    emotionLabels = getMatchingLabels(corporaData)\n",
    "    tokenFreq1 = getTokenFrequency(corpus1Text)\n",
    "    tokenFreq2 = getTokenFrequency(corpus2Text)\n",
    "    sim = getCosineSimilarityFromTokenFreq(tokenFreq1, tokenFreq2)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCosineSimilarityFromCorpusEmotions(corpus1,corpus2):\n",
    "    corpus1Text, corpus2Text, corpus1Data, corpus2Data = getTokenizedCorpusTextPair(corpus1, corpus2)\n",
    "    emotionLabels = getMatchingLabels(corporaData)\n",
    "    emotionDicts1 = getTop5000WordsByEmotion(corpus1Data, words, emotionLabels)\n",
    "    emotionDicts2 = getTop5000WordsByEmotion(corpus2Data, words, emotionLabels)\n",
    "    for emotion in range(len(emotionLabels)):\n",
    "        sim = getCosineSimilarityFromTokenFreq(emotionDicts1[emotion], emotionDicts2[emotion])\n",
    "        print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#averages the values the come from jensenshannon into a single value\n",
    "def getJensenShannonFromNPArrays(np1,np2):\n",
    "    js_pq = jensenshannon(np1, np2)\n",
    "    print(js_pq)\n",
    "    sumJS = 0\n",
    "    length = len(js_pq)\n",
    "    for x in js_pq:\n",
    "        if math.isnan(x): #assume nan values should be interpretted as 0\n",
    "            sumJS += 0\n",
    "        else:\n",
    "            sumJS += x\n",
    "    js = sumJS/length\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getChiSquare(observed,calculated):\n",
    "#     chiSquare = ((observed - calculated)**2)/calculated\n",
    "#     return chiSquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates Chi Square, or at least it would have if I had finished implementing\n",
    "#see paper for details on why it was not implemented\n",
    "# def getChiSquareFromTokenFreq(tokenFreq1, tokenFreq2):\n",
    "# columnTotal1 = sum(tokenFreq1.values())\n",
    "# columnTotal2 = sum(tokenFreq2.values())\n",
    "# intersection = tokenFreq1.keys() & tokenFreq2.keys()\n",
    "# rowTotals = {key: tokenFreq1.get(key, 0) + tokenFreq2.get(key, 0)\n",
    "#           for key in set(dict1) | set(dict2)}\n",
    "# grandTotal = columnTotal1 + columnTotal2\n",
    "# chiSquareTotal = 0\n",
    "# calculated1 = []\n",
    "# calculated2 = []\n",
    "# for item, rowTotal in rowTotals.items():\n",
    "#     calculated1[item] = (rowTotal * columnTotal1) / grandTotal\n",
    "#     calculated2[item] = (rowTotal * columnTotal2) / grandTotal\n",
    "# for item, value in calculated.items():\n",
    "#     getChiSquare(observed,calculated[item])\n",
    "#     calculated[item]\n",
    "#     return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n",
      "bag size 17756\n",
      "{'disgust', 'fear', 'sadness', 'anger', 'joy'}\n",
      "emotions\n",
      "7353\n",
      "6707\n",
      "8349\n",
      "8864\n",
      "6938\n",
      "emotions\n",
      "3401\n",
      "3118\n",
      "2523\n",
      "3339\n",
      "2527\n",
      "length 1 7353\n",
      "length 2 3401\n",
      "disgust 0.45165658636487677\n",
      "length 1 6707\n",
      "length 2 3118\n",
      "fear 0.43688992382789643\n",
      "length 1 8349\n",
      "length 2 2523\n",
      "sadness 0.4538342791036557\n",
      "length 1 8864\n",
      "length 2 3339\n",
      "anger 0.462297437783694\n",
      "length 1 6938\n",
      "length 2 2527\n",
      "joy 0.4439890786057688\n",
      "length 1 12661\n",
      "length 2 8888\n",
      "length 1 8888\n",
      "length 2 12661\n",
      "0.5107359175632199\n",
      "0.5107359175632199\n"
     ]
    }
   ],
   "source": [
    "#This is a validation of my corpus similarity metrics\n",
    "corpus1 = \"ssec\"\n",
    "corpus1Data = []\n",
    "corpus1Text = []\n",
    "corpus2 = \"isear\"\n",
    "corpus2Data = []\n",
    "corpus2Text = []\n",
    "with open(JSON) as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if data[\"source\"] == corpus1:\n",
    "            corpus1Text.append(tokenize(data[\"text\"]))\n",
    "            corpus1Data.append(data)\n",
    "        if data[\"source\"] == corpus2:\n",
    "            corpus2Text.append(tokenize(data[\"text\"]))\n",
    "            corpus2Data.append(data)\n",
    "print(\"loaded data\")\n",
    "combinedCorpus = corpus1Data + corpus2Data\n",
    "combinedCorpusText = corpus1Text + corpus2Text\n",
    "# tokenFreq = getTokenFrequency(corpus1Text)\n",
    "# print(\"tokenFreq\", tokenFreq)\n",
    "words = getTop5000Words(combinedCorpus)\n",
    "corporaData = [corpus1Data,corpus2Data]\n",
    "emotionLabels = list(getMatchingLabels(corporaData))\n",
    "emotions1 = getWordCountsByEmotion(corpus1Data, emotionLabels)\n",
    "# print(emotions1)\n",
    "emotions2 = getWordCountsByEmotion(corpus2Data, emotionLabels)\n",
    "# print(emotions2)\n",
    "for emotion in range(len(emotionLabels)):\n",
    "    sim = getCosineSimilarityFromTokenFreq(emotions1[emotion], emotions2[emotion])\n",
    "    print(emotionLabels[emotion], sim)\n",
    "fullCorpus1Words = getTokenFrequency(corpus1Text)\n",
    "fullCorpus2Words = getTokenFrequency(corpus2Text)\n",
    "sim1 = getCosineSimilarityFromTokenFreq(fullCorpus1Words, fullCorpus2Words)\n",
    "sim2 = getCosineSimilarityFromTokenFreq(fullCorpus2Words, fullCorpus1Words)\n",
    "print(sim1)\n",
    "print(sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# import numpy as np\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# print(corpus1Text[:5])\n",
    "# gen_docs = corpus1Text[:5]\n",
    "# dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "# # print(dictionary.token2id)\n",
    "# corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "# tf_idf = gensim.models.TfidfModel(corpus)\n",
    "# for doc in tf_idf [corpus]:\n",
    "#     print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "# sims = gensim.similarities.Similarity(\"../Ling506TermProject/\",tf_idf[corpus],\n",
    "#                                         num_features=len(dictionary))\n",
    "\n",
    "\n",
    "\n",
    "# file2_docs = [\"Mars is the fourth planet in our solar system.\",\n",
    "#         \"It is second-smallest planet in the Solar System after Mercury.\",\n",
    "#         \"Saturn is yellow planet.\"]\n",
    "# tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "# print(\"Number of documents:\",len(file2_docs))  \n",
    "# for line in file2_docs:\n",
    "#     query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "#     query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and create bag of words\n",
    "\n",
    "# # perform a similarity query against the corpus\n",
    "# query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "# # print(document_number, document_similarity)\n",
    "# print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "\n",
    "# sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "# print(sum_of_sims)\n",
    "\n",
    "# avg_sims = [] # array of averages\n",
    "\n",
    "\n",
    "# # for line in query documents\n",
    "# for line in file2_docs:\n",
    "#     # tokenize words\n",
    "#     query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "#     # create bag of words\n",
    "#     query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "#     # find similarity for each document\n",
    "#     query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "#     # print (document_number, document_similarity)\n",
    "#     print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "#     # calculate sum of similarities for each query doc\n",
    "#     sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "#     # calculate average of similarity for each query doc\n",
    "#     avg = sum_of_sims / len(file_docs)\n",
    "#     # print average of similarity for each query doc\n",
    "#     print(f'avg: {sum_of_sims / len(file_docs)}')\n",
    "#     # add average values into array\n",
    "#     avg_sims.append(avg)  \n",
    "# # calculate total average\n",
    "# total_avg = np.sum(avg_sims, dtype=np.float)\n",
    "# # round the value and multiply by 100 to format it as percentage\n",
    "# percentage_of_similarity = round(float(total_avg) * 100)\n",
    "# # if percentage is greater than 100\n",
    "# # that means documents are almost same\n",
    "# if percentage_of_similarity >= 100:\n",
    "#     percentage_of_similarity = 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bad attempt at using prebuilt functions for distancing\n",
    "# arr1 = np.load(\"ssec_emotion-causetrain_xNP.npy\")\n",
    "# arr2 = np.load(\"ssec_grounded_emotionstrain_xNP.npy\")\n",
    "# arr3 = np.load(\"isear_ssectest_xNP.npy\")\n",
    "# corpus1 = \"ssec\"\n",
    "# corpus1Data = []\n",
    "# corpus1Text = []\n",
    "# corpus2 = \"isear\"\n",
    "# corpus2Data = []\n",
    "# corpus2Text = []\n",
    "# with open(JSON) as f:\n",
    "#     for line in f:\n",
    "#         data = json.loads(line)\n",
    "#         if data[\"source\"] == corpus1:\n",
    "#             corpus1Data.append(data)\n",
    "#         if data[\"source\"] == corpus2:\n",
    "#             corpus2Data.append(data)\n",
    "# print(\"loaded data\")\n",
    "# words1 = getTop5000Words(corpus1Data)\n",
    "# print(words1)\n",
    "# words2 = getTop5000Words(corpus2Data)\n",
    "# for data in tqdm(corpus1Data):\n",
    "#     corpus1Text.append(get_vector(data[\"text\"], words1))\n",
    "# for data in tqdm(corpus1Data):\n",
    "#     corpus2Text.append(get_vector(data[\"text\"], words2))\n",
    "# # print(corpus1Text[:30])\n",
    "# print(np.array_equal(arr1,arr2))\n",
    "# print(np.array_equal(arr1,arr3))\n",
    "# print(cosine_similarity(arr1,arr3))\n",
    "# print(chi2_kernel(arr1,arr3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method is modified to track \n",
    "def make_arrays(train, test, words, labels, mode=\"multi\", all_vs=False):\n",
    "    emotions = {label: x for x, label in enumerate(labels)}\n",
    "    print(\"emotions in make_arrays: \", emotions)\n",
    "    train_x, train_y, test_x, test_y = [], [], [], []\n",
    "    \n",
    "    print(\"train raw text: \", sys.getsizeof(train)/1000000)\n",
    "\n",
    "    for data in tqdm(train):\n",
    "        # Discard examples where we don't have all selected emotions\n",
    "        if (mode == \"single\" or all_vs or all(data[\"emotions\"][emo] is not None for emo in labels)):\n",
    "            train_y.append(get_emotion(data[\"emotions\"], labels, emotions, mode))\n",
    "            train_x.append(get_vector(data[\"text\"], words))\n",
    "    for data in tqdm(test):\n",
    "        test_y.append(get_emotion(data[\"emotions\"], labels, emotions, mode))\n",
    "        test_x.append(get_vector(data[\"text\"], words))\n",
    "\n",
    "    print(\"train_x length \", len(train_x))\n",
    "    print(\"train_x dimension of element \", len(train_x[0]))\n",
    "    train_xSize = sys.getsizeof(train_x)/1000000\n",
    "    train_ySize = sys.getsizeof(train_y)/1000000\n",
    "    train_xLength = len(train_x)\n",
    "    train_yLength = len(train_y)\n",
    "    print(\"train_x (text) size RAW:\", train_xSize,\"megabytes\")\n",
    "    print(\"train_y (labels) size RAW:\", train_ySize,\"megabytes\")\n",
    "    test_xSize = sys.getsizeof(test_x)/1000000\n",
    "    test_ySize = sys.getsizeof(test_y)/1000000\n",
    "    test_xLength = len(test_x)\n",
    "    test_yLength = len(test_y)\n",
    "    print(\"test_x (text) size RAW:\", test_xSize,\"megabytes\")\n",
    "    print(\"test_y (labels) size RAW:\", test_ySize,\"megabytes\")\n",
    "\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    test_x = np.array(test_x)\n",
    "    test_y = np.array(test_y)\n",
    "    train_xNPSize = (train_x.nbytes)/1000000\n",
    "    train_yNPSize = (train_y.nbytes)/1000000\n",
    "    test_xNPSize = (test_x.nbytes)/1000000\n",
    "    test_yNPSize = (test_y.nbytes)/1000000\n",
    "    \n",
    "    print(\"saved test_y\")\n",
    "    print(\"train_x Size stays the same\", train_xSize == train_xNPSize)\n",
    "    print(\"train_y Size stays the same\", train_ySize == train_yNPSize)\n",
    "    print(\"test_x Size stays the same\", test_xSize == test_xNPSize)\n",
    "    print(\"test_y Size stays the same\", test_ySize == test_yNPSize)\n",
    "    print(\"train_xNPSize (text) size:\", train_xNPSize,\"megabytes\")\n",
    "    print(\"train_yNPSize (labels) size:\", train_yNPSize,\"megabytes\")\n",
    "    print(\"test_xNPSize (text) size:\", test_xNPSize,\"megabytes\")\n",
    "    print(\"test_yNPSize (labels) size:\", test_yNPSize,\"megabytes\")\n",
    "    print(\"train_xNP length \", len(train_x))\n",
    "    print(\"train_xNP dimension of element \", train_x.ndim)\n",
    "    print(\"train_xNP size \", train_x.size)\n",
    "    sizes = train_xNPSize, train_yNPSize, test_xNPSize, test_yNPSize\n",
    "    return train_x, train_y, test_x, test_y, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kept as part of classification definitions, prevents division by 0 errors\n",
    "def cheatydiv(x, y):\n",
    "    return math.nan if y == 0 else x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification reporting is kept the same for simplicity (ie, no need to reinvent the wheel)\n",
    "def classification_report_own_single(test_y, predict_y, labels):\n",
    "    reports = {}\n",
    "    num2emo = {i: label for i, label in enumerate(labels)}\n",
    "    decisions = defaultdict(Counter)\n",
    "    for t, p in zip(test_y, predict_y):\n",
    "        decisions[t][p] += 1\n",
    "    for label in decisions:\n",
    "        tp = decisions[label][label]\n",
    "        fp = sum(decisions[x][label] for x in decisions if x != label)\n",
    "        tn = sum(\n",
    "            decisions[x][y]\n",
    "            for x in decisions\n",
    "            for y in decisions[x]\n",
    "            if x != label and y != label\n",
    "        )\n",
    "        fn = sum(decisions[label][y] for y in decisions[label] if y != label)\n",
    "        precision = tp / (tp + fp) if tp + fp else math.nan\n",
    "        recall = tp / (tp + fn) if tp + fn else math.nan\n",
    "        f1 = 2 * cheatydiv((precision * recall), (precision + recall))\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        reports[num2emo[label]] = Report(precision, recall, accuracy, f1, tp, tn, fp, fn)\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification reporting is kept the same for simplicity (ie, no need to reinvent the wheel)\n",
    "def classification_report_own_multi(test_y, predict_y, labels):\n",
    "    reports = {}\n",
    "    num2emo = {i: label for i, label in enumerate(labels)}\n",
    "    emo2num = {label: i for i, label in enumerate(labels)}\n",
    "    decisions = defaultdict(Counter)\n",
    "    for label in labels:\n",
    "        tp = fp = tn = fn = 0\n",
    "        for t, p in zip(test_y, predict_y):\n",
    "            # decisions[t][p] += 1\n",
    "            tp += bool(t[emo2num[label]] and p[emo2num[label]])\n",
    "            fp += bool(p[emo2num[label]] and not t[emo2num[label]])\n",
    "            fn += bool(t[emo2num[label]] and not p[emo2num[label]])\n",
    "            tn += bool(not t[emo2num[label]] and not p[emo2num[label]])\n",
    "        precision = tp / (tp + fp) if tp + fp else math.nan\n",
    "        recall = tp / (tp + fn) if tp + fn else math.nan\n",
    "        f1 = 2 * cheatydiv((precision * recall), (precision + recall))\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        reports[label] = Report(precision, recall, accuracy, f1, tp, tn, fp, fn)\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification reporting is kept the same for simplicity (ie, no need to reinvent the wheel)\n",
    "def analyse_results(test_y, predict_y, labels, test, first, second, output, mode):\n",
    "    print(\"analyse_results\")\n",
    "    prefix = f\"{first}_vs_{second}_{mode}\"\n",
    "    fprefix = output + \"/\" + prefix\n",
    "    with open(fprefix + \".txt\", \"w\", encoding=\"utf-8\") as f, open(fprefix + \".json\", \"w\") as g:\n",
    "        print(\"hello\")\n",
    "        prec, reca, f1, supp = precision_recall_fscore_support(\n",
    "            test_y, predict_y, pos_label=None, average=\"micro\"\n",
    "        )\n",
    "        accuracy = accuracy_score(test_y, predict_y)\n",
    "        scoreNameArray = [(prec, \"Precision\"),(reca, \"Recall\"),(f1, \"F1-score\"),(accuracy, \"Accuracy\")]\n",
    "        for score, name in scoreNameArray:\n",
    "            print(name, score, sep=\"\\t\", file=f)\n",
    "            print(name, score, sep=\"\\t\")\n",
    "            \n",
    "        # print(\"real:\", Counter(test_y), file=f)\n",
    "        # print(\"predicted:\", Counter(predict_y), file=f)\n",
    "        \n",
    "        print(test_y[:10], predict_y[:10], file=f)\n",
    "        emotions = {i: label for i, label in enumerate(labels)}\n",
    "        for text, real, predicted, _ in zip(test, test_y, predict_y, range(20)):\n",
    "            if mode == \"multi\" and np.array_equal(real, predicted):\n",
    "                continue\n",
    "            elif mode == \"single\" and real == predicted:\n",
    "                continue\n",
    "            print(text, \"=> predicted:\", predicted, \", truth:\", real, file=f)\n",
    "        if mode == \"multi\":\n",
    "            results = classification_report_own_multi(test_y, predict_y, labels)\n",
    "        elif mode == \"single\":\n",
    "            results = classification_report_own_single(test_y, predict_y, labels)\n",
    "        json.dump(\n",
    "            {\n",
    "                \"precision\": prec,\n",
    "                \"recall\": reca,\n",
    "                \"f1\": f1,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"name\": prefix,\n",
    "                **{\n",
    "                    (emotion + \"_\" + metric): getattr(results[emotion], metric)\n",
    "                    for emotion in results\n",
    "                    for metric in Report._fields\n",
    "                },\n",
    "            },\n",
    "            g,\n",
    "        )\n",
    "        g.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for benchmarking/validating the results of the authors, but not in the final version\n",
    "#method is kept here for documentation\n",
    "def hacky_train_test_split(training, train_size=0.8, first=None, second=None):\n",
    "    tra, tes = [], []\n",
    "    for example in training:\n",
    "        if example.get(\"split\") == \"train\" or example[\"source\"] != second:\n",
    "            tra.append(example)\n",
    "        elif example.get(\"split\") == \"test\":\n",
    "            tes.append(example)\n",
    "        else:\n",
    "            # don't try this at home\n",
    "            [tes, tra][random.random()<train_size].append(example)\n",
    "    return tra, tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for benchmarking/validating the results of the authors, but not in the final version\n",
    "#method is kept here for documentation\n",
    "def splitTrainAndTestData(training, train_size=0.8, first=None, second=None):\n",
    "    tra, tes = [], []\n",
    "    for example in training:\n",
    "        if example.get(\"split\") == \"train\" or example[\"source\"] != second:\n",
    "            tra.append(example)\n",
    "        elif example.get(\"split\") == \"test\":\n",
    "            tes.append(example)\n",
    "        else:\n",
    "            # don't try this at home\n",
    "            [tes, tra][random.random()<train_size].append(example)\n",
    "    return tra, tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method is used in my testing to generate the combinations that I use in my trials automation\n",
    "def getPowerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method is used in my testing to generate the combinations that I use in my trials automation\n",
    "def getPermutations(s):\n",
    "    subsets = set()\n",
    "    for L in range(2, 3): #this \n",
    "        for subset in itertools.permutations(s, L):\n",
    "            subsets.add(subset)\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method is simply in place to get a measure of hard drive space left on my computer\n",
    "def getHardDriveSpaceLeft():\n",
    "    total, used, free = shutil.disk_usage(\"/\")\n",
    "    total = (total // (2**30))\n",
    "    used = (used // (2**30))\n",
    "    free = (free // (2**30))\n",
    "    print(\"Total: %d GB\" % total)\n",
    "    print(\"Used: %d GB\" % used)\n",
    "    print(\"Free: %d GB\" % free)\n",
    "    return total, used, free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCrossCorpusValuesWithOrder(possibleChoices):\n",
    "    #gets the runtime values for cross corpus trials\n",
    "    #ordering will matter if using the original authors version\n",
    "    permutations = list(getPermutations(possibleChoices))\n",
    "    print(\"permutations length: \",len(permutations))\n",
    "#         print(permutations)\n",
    "    corporaSets = []\n",
    "    for choice in permutations:\n",
    "#         print(\"choice \", choice)\n",
    "        if(len(choice) == 2):\n",
    "#             print(\"pair\")\n",
    "            first, second = choice\n",
    "            firstCorpus, domain1 = first\n",
    "            secondCorpus, domain2 = second\n",
    "        corpusPairData = (firstCorpus, secondCorpus, domain1, domain2)\n",
    "        corporaSets.append(corpusPairData)\n",
    "    return(corporaSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method adds the combinations relating to the ALl-VS trials\n",
    "def getAllVsCorpusValues(possibleChoices):\n",
    "    corporaSets = []\n",
    "    for entry in possibleChoices:\n",
    "        firstCorpus, domain1 = (None, None)\n",
    "        secondCorpus, domain2 = entry\n",
    "        corpusPairData = (firstCorpus, secondCorpus, domain1, domain2)\n",
    "        corporaSets.append(corpusPairData)\n",
    "    return corporaSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the corpora pairs of the same domain\n",
    "#powerSet is specified because it is only used in the case with a powerSet where ordering does not matter\n",
    "#but hypothetically, you could put in any list of possible entries\n",
    "def getCorporaPairsOfSameDomain(powerSet, sizeBoundLower=1, sizeBoundUpper=3):\n",
    "    for entry in powerSet:\n",
    "#       if len(entry) < 3 and len(entry) > 0:\n",
    "        if len(entry) < sizeBoundUpper and len(entry) > sizeBoundLower:\n",
    "            domainMatch = entry[0][1]\n",
    "            shouldAppend = True\n",
    "            for corpus, domain in entry:\n",
    "                if domain != domainMatch:\n",
    "                    shouldAppend = False\n",
    "            if(shouldAppend):\n",
    "                powerSetCondensed.append(entry)\n",
    "    print(\"CorporaPairsOfSameDomain:\",len(powerSetCondensed))\n",
    "    return sameDomainCorporaPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method adds the trials where the corpus is trained and tested on itself\n",
    "def getCorporaPairsWithItself(possibleChoices):\n",
    "    corporaSets = []\n",
    "    for entry in possibleChoices:\n",
    "        firstCorpus, domain1 = entry\n",
    "        secondCorpus, domain2 = entry\n",
    "        corpusPairData = (firstCorpus, secondCorpus, domain1, domain2)\n",
    "        corporaSets.append(corpusPairData)\n",
    "    return corporaSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getCorporaTriplets(possibleChoices):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performTrialUsingCorpusPair(corpusPair, verifyResults):\n",
    "    print(\"------------------------------\",corpusPair,\"-------------------------------------------\")\n",
    "    (first, second, domain1, domain2) = corpusPair\n",
    "    print(\"Getting data\")\n",
    "    jsonfile = \"unified-dataset.jsonl\"\n",
    "    output = \".\"\n",
    "    debug = True\n",
    "    forceMulti = False\n",
    "    isAllVS = False\n",
    "    if first == None:\n",
    "        isAllVS = True\n",
    "    \n",
    "    if(verifyResults == False):\n",
    "        train_xNPFileName = first + \"_\" + second + \"train_xNP\" +\".npy\"\n",
    "        train_yNPFileName = first + \"_\" + second + \"train_yNP\" +\".npy\"\n",
    "        test_xNPFileName = first + \"_\" + second + \"test_xNP\" +\".npy\"\n",
    "        test_yNPFileName = first + \"_\" + second + \"test_yNP\" +\".npy\"\n",
    "        classifierName = \"RandomForestClassifier\"\n",
    "        classiferSaveFile = first+\"_\"+second+classifierName+\".pkl\"\n",
    "        filesValid = (path.exists(train_xNPFileName) \n",
    "                       and path.exists(train_yNPFileName)\n",
    "                       and path.exists(test_xNPFileName)\n",
    "                       and path.exists(test_yNPFileName)\n",
    "                       and path.exists(classiferSaveFile))\n",
    "        print(\"do pickle files exist?\", filesValid)\n",
    "        if(filesValid):\n",
    "            print(\"skipping trial\")\n",
    "            return\n",
    "    \n",
    "    \n",
    "    training_data, testing_data = get_train_test(jsonfile, first,second)\n",
    "    firstCLF, secondCLF = ([\"multi\", \"multi\"] if forceMulti else get_clf_mode(training_data, testing_data))\n",
    "    mode = \"multi\" if \"multi\" in [firstCLF, secondCLF] else \"single\"\n",
    "\n",
    "    print(\"Detected mode: {}...\".format(mode))\n",
    "    print(len(training_data), len(testing_data))\n",
    "    print(\"Getting wordlist...\")\n",
    "    if debug:\n",
    "        wordlist = getTop5000Words(training_data)\n",
    "    else:\n",
    "        wordlist = getTop5000Words(training_data)\n",
    "        # wordlist = get_wordlist(training_data)\n",
    "    print(\"Getting emotions\")\n",
    "    labels = get_labels(training_data, testing_data, mode=mode)\n",
    "    print(labels)\n",
    "    print(\"Making arrays\")\n",
    "    print(\"checking for save files\")\n",
    "    if(first == None):\n",
    "        first = \"all-vs\"\n",
    "    train_xNPFileName = first + \"_\" + second + \"train_xNP\" +\".npy\"\n",
    "    train_yNPFileName = first + \"_\" + second + \"train_yNP\" +\".npy\"\n",
    "    test_xNPFileName = first + \"_\" + second + \"test_xNP\" +\".npy\"\n",
    "    test_yNPFileName = first + \"_\" + second + \"test_yNP\" +\".npy\"\n",
    "\n",
    "    if(path.exists(train_xNPFileName) \n",
    "       and path.exists(train_yNPFileName)\n",
    "       and path.exists(test_xNPFileName)\n",
    "       and path.exists(test_yNPFileName)):\n",
    "        print('saved train_xNP as', train_xNPFileName)\n",
    "        print('saved train_yNP as', train_yNPFileName)\n",
    "        print('saved test_xNP as', test_xNPFileName)\n",
    "        print('saved test_yNP as', test_yNPFileName)\n",
    "        print(\"loading from np\")\n",
    "        train_x = np.load(train_xNPFileName)\n",
    "        train_y = np.load(train_yNPFileName)\n",
    "        test_x = np.load(test_xNPFileName)\n",
    "        test_y = np.load(test_yNPFileName)\n",
    "        train_xNPSize = (train_x.nbytes)/1000000\n",
    "        train_yNPSize = (train_y.nbytes)/1000000\n",
    "        test_xNPSize = (test_x.nbytes)/1000000\n",
    "        test_yNPSize = (test_y.nbytes)/1000000\n",
    "        print(\"loaded directly from NP.load\")\n",
    "        print(\"train_xNPSize (text) size loaded:\", train_xNPSize,\"megabytes\")\n",
    "        print(\"train_yNPSize (labels) size loaded:\", train_yNPSize,\"megabytes\")\n",
    "        print(\"test_xNPSize (text) size loaded:\", test_xNPSize,\"megabytes\")\n",
    "        print(\"test_yNPSize (labels) size loaded:\", test_yNPSize,\"megabytes\")\n",
    "    else:\n",
    "#         print(\"training_data\", training_data)\n",
    "#         print(\"testing_data\", testing_data)\n",
    "        train_x, train_y, test_x, test_y, sizes = make_arrays(training_data, testing_data, wordlist, labels, mode, isAllVS)\n",
    "        train_xSize, train_ySize, test_xSize, test_ySize = sizes\n",
    "        if any(not part.size for part in [train_x, train_y, test_x, test_y]):\n",
    "            print(\"Train or test empty. Did you misspell the dataset name?\")\n",
    "            return\n",
    "        #             sys.exit(1)\n",
    "        print(\"saving NP arrays\")\n",
    "        np.save(train_xNPFileName, train_x)\n",
    "        np.save(train_yNPFileName, train_y)\n",
    "        np.save(test_xNPFileName, test_x)\n",
    "        np.save(test_yNPFileName, test_y)\n",
    "        print(\"NP arrays saved\")\n",
    "\n",
    "    print(\"Initializing classifier\")\n",
    "    trainClassifier = True\n",
    "    if debug:\n",
    "        classifierName = \"RandomForestClassifier\"\n",
    "        print(\"Searching for a \", classifierName)\n",
    "        classiferSaveFile = first+\"_\"+second+classifierName+\".pkl\"\n",
    "        print(path.exists(classiferSaveFile))\n",
    "        if(path.exists(classiferSaveFile)):\n",
    "            trainClassifier = False\n",
    "            print(\"Loading classifier from file\")\n",
    "            classifier = joblib.load(classiferSaveFile)\n",
    "            print(\"classifier loaded successfully\")\n",
    "        else:\n",
    "            print(\"file not found, creating new classifier\")\n",
    "            classifier = RandomForestClassifier()\n",
    "    elif mode == \"single\":\n",
    "        classifierName = \"LogisticRegressionCV\"\n",
    "        print(\"Searching for a \", classifierName)\n",
    "        classiferSaveFile = first+\"_\"+second+classifierName+\".pkl\"\n",
    "        print(path.exists(classiferSaveFile))\n",
    "        if(path.exists(classiferSaveFile)):\n",
    "            trainClassifier = False\n",
    "            print(\"Loading classifier from file\")\n",
    "            classifier = joblib.load(classiferSaveFile)\n",
    "            print(\"classifier loaded successfully\")\n",
    "        else:\n",
    "            print(\"file not found, creating new classifier\")\n",
    "            classifier = LogisticRegressionCV(\n",
    "                cv=10,\n",
    "                penalty=\"l2\",\n",
    "                fit_intercept=True,\n",
    "                solver=\"sag\",\n",
    "                scoring=\"f1\",\n",
    "                refit=True,\n",
    "                # n_jobs=-1,\n",
    "                class_weight=\"balanced\",\n",
    "            )\n",
    "    else:\n",
    "        classifierName = \"OneVsRestClassifier\"\n",
    "        print(\"Searching for a \", classifierName)\n",
    "        classiferSaveFile = first+\"_\"+second+classifierName+\".pkl\"\n",
    "        print(path.exists(classiferSaveFile))\n",
    "        if(path.exists(classiferSaveFile)):\n",
    "            trainClassifier = False\n",
    "            print(\"Loading classifier from file\")\n",
    "            classifier = joblib.load(classiferSaveFile)\n",
    "            print(\"classifier loaded successfully\")\n",
    "        else:\n",
    "            print(\"file not found, creating new classifier\")\n",
    "            classifier = OneVsRestClassifier(\n",
    "                LogisticRegressionCV(\n",
    "                    cv=10,\n",
    "                    penalty=\"l2\",\n",
    "                    fit_intercept=True,\n",
    "                    solver=\"sag\",\n",
    "                    scoring=\"f1\",\n",
    "                    refit=True,\n",
    "                    class_weight=\"balanced\",\n",
    "                    tol = 0.1,\n",
    "                ),\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "    if(trainClassifier):\n",
    "        print(\"this is the classifierName: \", classifierName)\n",
    "        print(\"Training...\")\n",
    "        print(\"train_x (text) size:\", (train_x.nbytes)/1000000,\"megabytes\")\n",
    "        print(\"train_y (labels) size:\", (train_y.nbytes)/1000000,\"megabytes\")\n",
    "        print(\"train_x (text) length:\", len(train_x))\n",
    "        print(\"train_y (labels) length:\", len(train_y))\n",
    "        print(train_x[:5])\n",
    "        print(train_y[:5])\n",
    "\n",
    "        classifier.fit(train_x, train_y)\n",
    "        print(\"finished training, classifier size:\", sys.getsizeof(classifier)/1000000,\"megabytes\")\n",
    "    print(\"Predicting...\")\n",
    "    if first == \"multi\" and second == \"single\":\n",
    "        predict_y = classifier.predict_proba(test_x)\n",
    "        helper = np.zeros_like(predict_y)\n",
    "        helper[range(len(predict_y)), predict_y.argmax(1)] = 1\n",
    "        predict_y = helper\n",
    "    else:\n",
    "        predict_y = classifier.predict(test_x)\n",
    "\n",
    "    print(\"Analysing...\")\n",
    "\n",
    "    analyse_results(\n",
    "        test_y,\n",
    "        predict_y,\n",
    "        labels,\n",
    "        testing_data,\n",
    "        first,\n",
    "        second,\n",
    "        output,\n",
    "        mode,  # TODO\n",
    "    )\n",
    "    if(path.exists(classiferSaveFile)):\n",
    "        print(\"classifier already saved\")\n",
    "    else:\n",
    "#         classiferSaveFile = first+\"_\"+second+classifierName+\".pkl\"\n",
    "        print(\"classiferSaveFile: \", classiferSaveFile)\n",
    "        joblib.dump(classifier, classiferSaveFile)\n",
    "        print(\"Saved Successfully\")\n",
    "    total, used, free = getHardDriveSpaceLeft()\n",
    "    if(free < 10):\n",
    "        sys.exit(\"Error: less than 10 gb remaining on disk\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTrials(version, verifyResults, crossCorpus=True, sameCorpus=True, allVs=False):\n",
    "    possibleChoices = [('affectivetext','headlines'), ('crowdflower','tweets'), ('dailydialog','conversations'), \n",
    "                       ('emoint','tweets'), ('emotion-cause','paragraphs'), ('grounded_emotions','tweets'), \n",
    "                       ('isear','descriptions'), ('ssec','tweets'),('tales-emotion','tales'), ('tec','tweets')]\n",
    "                        #excluded ('emobank','headlines') because it is isn't emotion annotated\n",
    "                        #and ('electoraltweets','tweets') because it has incompatible annotation\n",
    "                        #and  ('fb-valence-arousal-anon','tweets') because it isn't emotion annotated\n",
    "    corporaSets = []\n",
    "    if version == \"previous\":\n",
    "        corporaSets = (getCrossCorpusValuesWithOrder(possibleChoices))\n",
    "        #this was added to sort the lists by domain of the first, then by the first corpus name, then the second.\n",
    "        #it is placed in reverse order simply because if it was put in regular order, the largest of the trials would be first\n",
    "        #sorting in reverse will (loosely) make the smaller trials run first, while having no impact on the ability to obtain all results\n",
    "        sortedPermutations = sorted(corporaSets, key = lambda x: (x[2], x[0], x[1]), reverse = True)\n",
    "        sortedPermutations += (getCorporaPairsWithItself(possibleChoices))\n",
    "        sortedPermutations += (getAllVsCorpusValues(possibleChoices))\n",
    "        for corpusPair in sortedPermutations:\n",
    "            performTrialUsingCorpusPair(corpusPair, verifyResults)\n",
    "    else: #version == \"myTrials\"\n",
    "        powerSet = list(getPowerset(possibleChoices))\n",
    "    \n",
    "    print(\"End of program!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutations length:  90\n",
      "------------------------------ ('tec', 'tales-emotion', 'tweets', 'tales') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tec', 'ssec', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tec', 'isear', 'tweets', 'descriptions') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tec', 'grounded_emotions', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tec', 'emotion-cause', 'tweets', 'paragraphs') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tec', 'emoint', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tec', 'dailydialog', 'tweets', 'conversations') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tec', 'crowdflower', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tec', 'affectivetext', 'tweets', 'headlines') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('ssec', 'tec', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('ssec', 'tales-emotion', 'tweets', 'tales') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('ssec', 'isear', 'tweets', 'descriptions') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('ssec', 'grounded_emotions', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('ssec', 'emotion-cause', 'tweets', 'paragraphs') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('ssec', 'emoint', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('ssec', 'dailydialog', 'tweets', 'conversations') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('ssec', 'crowdflower', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('ssec', 'affectivetext', 'tweets', 'headlines') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('grounded_emotions', 'tec', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('grounded_emotions', 'tales-emotion', 'tweets', 'tales') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('grounded_emotions', 'ssec', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('grounded_emotions', 'isear', 'tweets', 'descriptions') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('grounded_emotions', 'emotion-cause', 'tweets', 'paragraphs') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('grounded_emotions', 'emoint', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('grounded_emotions', 'dailydialog', 'tweets', 'conversations') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('grounded_emotions', 'crowdflower', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('grounded_emotions', 'affectivetext', 'tweets', 'headlines') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emoint', 'tec', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emoint', 'tales-emotion', 'tweets', 'tales') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emoint', 'ssec', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emoint', 'isear', 'tweets', 'descriptions') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emoint', 'grounded_emotions', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emoint', 'emotion-cause', 'tweets', 'paragraphs') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emoint', 'dailydialog', 'tweets', 'conversations') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emoint', 'crowdflower', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emoint', 'affectivetext', 'tweets', 'headlines') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('crowdflower', 'tec', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('crowdflower', 'tales-emotion', 'tweets', 'tales') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('crowdflower', 'ssec', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('crowdflower', 'isear', 'tweets', 'descriptions') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('crowdflower', 'grounded_emotions', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('crowdflower', 'emotion-cause', 'tweets', 'paragraphs') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('crowdflower', 'emoint', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('crowdflower', 'dailydialog', 'tweets', 'conversations') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('crowdflower', 'affectivetext', 'tweets', 'headlines') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ ('tales-emotion', 'tec', 'tales', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tales-emotion', 'ssec', 'tales', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tales-emotion', 'isear', 'tales', 'descriptions') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tales-emotion', 'grounded_emotions', 'tales', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tales-emotion', 'emotion-cause', 'tales', 'paragraphs') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tales-emotion', 'emoint', 'tales', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tales-emotion', 'dailydialog', 'tales', 'conversations') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tales-emotion', 'crowdflower', 'tales', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('tales-emotion', 'affectivetext', 'tales', 'headlines') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emotion-cause', 'tec', 'paragraphs', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emotion-cause', 'tales-emotion', 'paragraphs', 'tales') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emotion-cause', 'ssec', 'paragraphs', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emotion-cause', 'isear', 'paragraphs', 'descriptions') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emotion-cause', 'grounded_emotions', 'paragraphs', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emotion-cause', 'emoint', 'paragraphs', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emotion-cause', 'dailydialog', 'paragraphs', 'conversations') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emotion-cause', 'crowdflower', 'paragraphs', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('emotion-cause', 'affectivetext', 'paragraphs', 'headlines') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('affectivetext', 'tec', 'headlines', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('affectivetext', 'tales-emotion', 'headlines', 'tales') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('affectivetext', 'ssec', 'headlines', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('affectivetext', 'isear', 'headlines', 'descriptions') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('affectivetext', 'grounded_emotions', 'headlines', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('affectivetext', 'emotion-cause', 'headlines', 'paragraphs') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('affectivetext', 'emoint', 'headlines', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('affectivetext', 'dailydialog', 'headlines', 'conversations') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('affectivetext', 'crowdflower', 'headlines', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('isear', 'tec', 'descriptions', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('isear', 'tales-emotion', 'descriptions', 'tales') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('isear', 'ssec', 'descriptions', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('isear', 'grounded_emotions', 'descriptions', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('isear', 'emotion-cause', 'descriptions', 'paragraphs') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('isear', 'emoint', 'descriptions', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('isear', 'dailydialog', 'descriptions', 'conversations') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('isear', 'crowdflower', 'descriptions', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('isear', 'affectivetext', 'descriptions', 'headlines') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('dailydialog', 'tec', 'conversations', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('dailydialog', 'tales-emotion', 'conversations', 'tales') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('dailydialog', 'ssec', 'conversations', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('dailydialog', 'isear', 'conversations', 'descriptions') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('dailydialog', 'grounded_emotions', 'conversations', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('dailydialog', 'emotion-cause', 'conversations', 'paragraphs') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('dailydialog', 'emoint', 'conversations', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('dailydialog', 'crowdflower', 'conversations', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ ('dailydialog', 'affectivetext', 'conversations', 'headlines') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('affectivetext', 'affectivetext', 'headlines', 'headlines') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? True\n",
      "skipping trial\n",
      "------------------------------ ('crowdflower', 'crowdflower', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? False\n",
      "get_train_test param:\n",
      "json  unified-dataset.jsonl\n",
      "train  crowdflower\n",
      "test  crowdflower\n",
      "there were  39740  entries that were in test and  181699 that were not in test and  0  that were in train\n",
      "test was appended  39740  times\n",
      "revised there were  31726  entries that were in test and  8014  that were in train\n",
      "single\n",
      "oof\n",
      "Detected mode: single...\n",
      "31726 8014\n",
      "Getting wordlist...\n",
      "bag size 38468\n",
      "Getting emotions\n",
      "['noemo', 'disgust', 'anger', 'surprise', 'love', 'sadness', 'fear', 'joy']\n",
      "Making arrays\n",
      "checking for save files\n",
      "saved train_xNP as crowdflower_crowdflowertrain_xNP.npy\n",
      "saved train_yNP as crowdflower_crowdflowertrain_yNP.npy\n",
      "saved test_xNP as crowdflower_crowdflowertest_xNP.npy\n",
      "saved test_yNP as crowdflower_crowdflowertest_yNP.npy\n",
      "loading from np\n",
      "loaded directly from NP.load\n",
      "train_xNPSize (text) size loaded: 633.58 megabytes\n",
      "train_yNPSize (labels) size loaded: 0.126716 megabytes\n",
      "test_xNPSize (text) size loaded: 161.22 megabytes\n",
      "test_yNPSize (labels) size loaded: 0.032244 megabytes\n",
      "Initializing classifier\n",
      "Searching for a  RandomForestClassifier\n",
      "False\n",
      "file not found, creating new classifier\n",
      "this is the classifierName:  RandomForestClassifier\n",
      "Training...\n",
      "train_x (text) size: 633.58 megabytes\n",
      "train_y (labels) size: 0.126716 megabytes\n",
      "train_x (text) length: 31679\n",
      "train_y (labels) length: 31679\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n",
      "[0 5 5 7 0]\n",
      "finished training, classifier size: 4.8e-05 megabytes\n",
      "Predicting...\n",
      "Analysing...\n",
      "analyse_results\n",
      "hello\n",
      "Precision\t0.3897779431832279\n",
      "Recall\t0.3897779431832279\n",
      "F1-score\t0.38977794318322784\n",
      "Accuracy\t0.3897779431832279\n",
      "classiferSaveFile:  crowdflower_crowdflowerRandomForestClassifier.pkl\n",
      "Saved Successfully\n",
      "Total: 475 GB\n",
      "Used: 370 GB\n",
      "Free: 104 GB\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------------ ('dailydialog', 'dailydialog', 'conversations', 'conversations') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? False\n",
      "get_train_test param:\n",
      "json  unified-dataset.jsonl\n",
      "train  dailydialog\n",
      "test  dailydialog\n",
      "there were  102979  entries that were in test and  118460 that were not in test and  0  that were in train\n",
      "test was appended  102979  times\n",
      "revised there were  82213  entries that were in test and  20766  that were in train\n",
      "single\n",
      "oof\n",
      "Detected mode: single...\n",
      "82213 20766\n",
      "Getting wordlist...\n",
      "bag size 16697\n",
      "Getting emotions\n",
      "['noemo', 'disgust', 'anger', 'surprise', 'sadness', 'fear', 'joy']\n",
      "Making arrays\n",
      "checking for save files\n",
      "emotions in make_arrays:  {'noemo': 0, 'disgust': 1, 'anger': 2, 'surprise': 3, 'sadness': 4, 'fear': 5, 'joy': 6}\n",
      "train raw text:  0.732808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/82213 [00:00<?, ?it/s]\n",
      "  0%|                                                                           | 166/82213 [00:00<00:49, 1649.45it/s]\n",
      "  0%|                                                                           | 406/82213 [00:00<00:45, 1817.58it/s]\n",
      "  1%|                                                                           | 634/82213 [00:00<00:42, 1932.18it/s]\n",
      "  1%|                                                                           | 892/82213 [00:00<00:38, 2086.34it/s]\n",
      "  1%|                                                                          | 1181/82213 [00:00<00:35, 2272.80it/s]\n",
      "  2%|                                                                         | 1465/82213 [00:00<00:33, 2413.55it/s]\n",
      "  2%|                                                                         | 1775/82213 [00:00<00:31, 2581.33it/s]\n",
      "  3%|                                                                         | 2065/82213 [00:00<00:30, 2664.62it/s]\n",
      "  3%|                                                                        | 2388/82213 [00:00<00:28, 2807.61it/s]\n",
      "  3%|                                                                        | 2706/82213 [00:01<00:27, 2904.73it/s]\n",
      "  4%|                                                                        | 2998/82213 [00:01<00:27, 2869.95it/s]\n",
      "  4%|                                                                        | 3299/82213 [00:01<00:27, 2905.19it/s]\n",
      "  4%|                                                                       | 3591/82213 [00:01<00:27, 2870.03it/s]\n",
      "  5%|                                                                       | 3893/82213 [00:01<00:26, 2908.27it/s]\n",
      "  5%|                                                                       | 4185/82213 [00:01<00:29, 2606.72it/s]\n",
      "  5%|                                                                       | 4493/82213 [00:01<00:28, 2728.05it/s]\n",
      "  6%|                                                                      | 4810/82213 [00:01<00:27, 2842.22it/s]\n",
      "  6%|                                                                      | 5100/82213 [00:02<00:45, 1691.17it/s]\n",
      "  7%|                                                                      | 5420/82213 [00:02<00:39, 1967.49it/s]\n",
      "  7%|                                                                     | 5676/82213 [00:02<00:36, 2085.62it/s]\n",
      "  7%|                                                                     | 5974/82213 [00:02<00:33, 2288.61it/s]\n",
      "  8%|                                                                     | 6239/82213 [00:02<00:32, 2344.68it/s]\n",
      "  8%|                                                                     | 6500/82213 [00:02<00:32, 2362.40it/s]\n",
      "  8%|                                                                    | 6757/82213 [00:02<00:31, 2416.69it/s]\n",
      "  9%|                                                                    | 7055/82213 [00:02<00:29, 2557.77it/s]\n",
      "  9%|                                                                    | 7374/82213 [00:02<00:27, 2714.82it/s]\n",
      "  9%|                                                                    | 7672/82213 [00:03<00:26, 2784.48it/s]\n",
      " 10%|                                                                   | 7959/82213 [00:03<00:26, 2772.27it/s]\n",
      " 10%|                                                                   | 8242/82213 [00:03<00:26, 2775.98it/s]\n",
      " 10%|                                                                   | 8534/82213 [00:03<00:26, 2812.47it/s]\n",
      " 11%|                                                                   | 8853/82213 [00:03<00:25, 2910.68it/s]\n",
      " 11%|                                                                  | 9147/82213 [00:03<00:25, 2896.80it/s]\n",
      " 11%|                                                                  | 9439/82213 [00:03<00:25, 2898.48it/s]\n",
      " 12%|                                                                  | 9731/82213 [00:03<00:26, 2738.40it/s]\n",
      " 12%|                                                                 | 10049/82213 [00:03<00:25, 2852.51it/s]\n",
      " 13%|                                                                | 10338/82213 [00:03<00:25, 2833.12it/s]\n",
      " 13%|                                                                | 10644/82213 [00:04<00:24, 2892.53it/s]\n",
      " 13%|                                                                | 10936/82213 [00:04<00:26, 2711.91it/s]\n",
      " 14%|                                                                | 11211/82213 [00:04<00:27, 2553.88it/s]\n",
      " 14%|                                                               | 11518/82213 [00:04<00:26, 2685.02it/s]\n",
      " 14%|                                                               | 11793/82213 [00:04<00:26, 2699.10it/s]\n",
      " 15%|                                                               | 12090/82213 [00:04<00:25, 2770.07it/s]\n",
      " 15%|                                                              | 12371/82213 [00:04<00:25, 2776.64it/s]\n",
      " 15%|                                                              | 12655/82213 [00:04<00:24, 2789.86it/s]\n",
      " 16%|                                                              | 12965/82213 [00:04<00:24, 2871.29it/s]\n",
      " 16%|                                                              | 13254/82213 [00:05<00:31, 2218.83it/s]\n",
      " 16%|                                                             | 13500/82213 [00:05<00:32, 2119.08it/s]\n",
      " 17%|                                                             | 13730/82213 [00:05<00:31, 2142.48it/s]\n",
      " 17%|                                                             | 13962/82213 [00:05<00:31, 2188.84it/s]\n",
      " 17%|                                                             | 14229/82213 [00:05<00:29, 2310.10it/s]\n",
      " 18%|                                                             | 14524/82213 [00:05<00:27, 2466.75it/s]\n",
      " 18%|                                                            | 14785/82213 [00:05<00:26, 2503.63it/s]\n",
      " 18%|                                                            | 15042/82213 [00:05<00:28, 2367.36it/s]\n",
      " 19%|                                                            | 15310/82213 [00:05<00:27, 2448.91it/s]\n",
      " 19%|                                                            | 15610/82213 [00:06<00:25, 2587.26it/s]\n",
      " 19%|                                                           | 15895/82213 [00:06<00:24, 2656.31it/s]\n",
      " 20%|                                                           | 16165/82213 [00:06<00:26, 2462.78it/s]\n",
      " 20%|                                                           | 16453/82213 [00:06<00:25, 2570.09it/s]\n",
      " 20%|                                                           | 16716/82213 [00:06<00:26, 2503.06it/s]\n",
      " 21%|                                                          | 17034/82213 [00:06<00:24, 2669.50it/s]\n",
      " 21%|                                                          | 17307/82213 [00:06<00:24, 2666.55it/s]\n",
      " 21%|                                                          | 17600/82213 [00:06<00:23, 2735.79it/s]\n",
      " 22%|                                                          | 17887/82213 [00:06<00:23, 2769.37it/s]\n",
      " 22%|                                                         | 18167/82213 [00:07<00:24, 2641.75it/s]\n",
      " 22%|                                                         | 18435/82213 [00:07<00:24, 2609.90it/s]\n",
      " 23%|                                                         | 18700/82213 [00:07<00:24, 2616.84it/s]\n",
      " 23%|                                                         | 18964/82213 [00:07<00:26, 2387.41it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|                                                        | 19208/82213 [00:07<00:27, 2324.04it/s]\n",
      " 24%|                                                        | 19445/82213 [00:07<00:27, 2241.76it/s]\n",
      " 24%|                                                        | 19673/82213 [00:07<00:28, 2229.32it/s]\n",
      " 24%|                                                        | 19899/82213 [00:07<00:30, 2010.61it/s]\n",
      " 24%|                                                        | 20121/82213 [00:07<00:30, 2065.46it/s]\n",
      " 25%|                                                       | 20412/82213 [00:08<00:27, 2259.07it/s]\n",
      " 25%|                                                       | 20696/82213 [00:08<00:25, 2402.86it/s]\n",
      " 25%|                                                       | 20945/82213 [00:08<00:26, 2273.17it/s]\n",
      " 26%|                                                       | 21180/82213 [00:08<00:29, 2064.92it/s]\n",
      " 26%|                                                      | 21396/82213 [00:08<00:29, 2053.41it/s]\n",
      " 26%|                                                      | 21608/82213 [00:08<00:39, 1543.05it/s]\n",
      " 27%|                                                      | 21813/82213 [00:08<00:36, 1664.10it/s]\n",
      " 27%|                                                      | 22058/82213 [00:08<00:32, 1838.63it/s]\n",
      " 27%|                                                      | 22313/82213 [00:09<00:29, 2003.55it/s]\n",
      " 27%|                                                     | 22583/82213 [00:09<00:27, 2168.24it/s]\n",
      " 28%|                                                     | 22830/82213 [00:09<00:26, 2246.81it/s]\n",
      " 28%|                                                     | 23067/82213 [00:09<00:27, 2178.97it/s]\n",
      " 28%|                                                     | 23294/82213 [00:09<00:28, 2037.63it/s]\n",
      " 29%|                                                    | 23580/82213 [00:09<00:26, 2226.52it/s]\n",
      " 29%|                                                    | 23902/82213 [00:09<00:23, 2450.06it/s]\n",
      " 29%|                                                    | 24190/82213 [00:09<00:22, 2547.32it/s]\n",
      " 30%|                                                    | 24484/82213 [00:09<00:21, 2649.06it/s]\n",
      " 30%|                                                   | 24787/82213 [00:09<00:20, 2748.10it/s]\n",
      " 30%|                                                   | 25069/82213 [00:10<00:21, 2656.17it/s]\n",
      " 31%|                                                   | 25341/82213 [00:10<00:21, 2631.74it/s]\n",
      " 31%|                                                   | 25609/82213 [00:10<00:21, 2610.55it/s]\n",
      " 31%|                                                  | 25873/82213 [00:10<00:21, 2606.50it/s]\n",
      " 32%|                                                  | 26136/82213 [00:10<00:23, 2346.89it/s]\n",
      " 32%|                                                  | 26377/82213 [00:10<00:24, 2313.88it/s]\n",
      " 32%|                                                  | 26659/82213 [00:10<00:22, 2441.66it/s]\n",
      " 33%|                                                 | 26946/82213 [00:10<00:21, 2551.56it/s]\n",
      " 33%|                                                 | 27218/82213 [00:10<00:21, 2595.30it/s]\n",
      " 33%|                                                 | 27502/82213 [00:11<00:20, 2659.37it/s]\n",
      " 34%|                                                 | 27771/82213 [00:11<00:21, 2474.83it/s]\n",
      " 34%|                                                | 28024/82213 [00:11<00:21, 2486.43it/s]\n",
      " 34%|                                                | 28312/82213 [00:11<00:20, 2588.25it/s]\n",
      " 35%|                                                | 28585/82213 [00:11<00:20, 2624.15it/s]\n",
      " 35%|                                                | 28875/82213 [00:11<00:19, 2696.61it/s]\n",
      " 35%|                                               | 29158/82213 [00:11<00:19, 2730.23it/s]\n",
      " 36%|                                               | 29433/82213 [00:11<00:19, 2699.17it/s]\n",
      " 36%|                                               | 29724/82213 [00:11<00:19, 2753.91it/s]\n",
      " 36%|                                               | 30001/82213 [00:12<00:24, 2147.01it/s]\n",
      " 37%|                                              | 30280/82213 [00:12<00:22, 2302.97it/s]\n",
      " 37%|                                              | 30529/82213 [00:12<00:22, 2338.53it/s]\n",
      " 37%|                                              | 30812/82213 [00:12<00:20, 2463.13it/s]\n",
      " 38%|                                              | 31070/82213 [00:12<00:21, 2351.23it/s]\n",
      " 38%|                                             | 31314/82213 [00:12<00:23, 2161.22it/s]\n",
      " 38%|                                             | 31615/82213 [00:12<00:21, 2357.26it/s]\n",
      " 39%|                                             | 31875/82213 [00:12<00:20, 2420.85it/s]\n",
      " 39%|                                             | 32167/82213 [00:12<00:19, 2547.62it/s]\n",
      " 39%|                                            | 32474/82213 [00:13<00:18, 2679.97it/s]\n",
      " 40%|                                            | 32750/82213 [00:13<00:18, 2690.84it/s]\n",
      " 40%|                                            | 33037/82213 [00:13<00:17, 2737.16it/s]\n",
      " 41%|                                            | 33315/82213 [00:13<00:18, 2674.33it/s]\n",
      " 41%|                                           | 33586/82213 [00:13<00:18, 2672.02it/s]\n",
      " 41%|                                           | 33901/82213 [00:13<00:17, 2794.48it/s]\n",
      " 42%|                                           | 34184/82213 [00:13<00:17, 2759.32it/s]\n",
      " 42%|                                           | 34463/82213 [00:13<00:17, 2669.49it/s]\n",
      " 42%|                                          | 34733/82213 [00:13<00:17, 2642.49it/s]\n",
      " 43%|                                          | 35034/82213 [00:13<00:17, 2738.18it/s]\n",
      " 43%|                                          | 35310/82213 [00:14<00:17, 2631.23it/s]\n",
      " 43%|                                          | 35576/82213 [00:14<00:18, 2516.71it/s]\n",
      " 44%|                                         | 35877/82213 [00:14<00:17, 2642.38it/s]\n",
      " 44%|                                         | 36145/82213 [00:14<00:17, 2617.89it/s]\n",
      " 44%|                                         | 36410/82213 [00:14<00:18, 2477.12it/s]\n",
      " 45%|                                         | 36690/82213 [00:14<00:17, 2561.39it/s]\n",
      " 45%|                                        | 36950/82213 [00:14<00:17, 2545.63it/s]\n",
      " 45%|                                        | 37207/82213 [00:14<00:17, 2511.40it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|                                        | 37460/82213 [00:14<00:18, 2433.47it/s]\n",
      " 46%|                                        | 37738/82213 [00:15<00:17, 2523.42it/s]\n",
      " 46%|                                       | 38039/82213 [00:15<00:16, 2647.75it/s]\n",
      " 47%|                                       | 38307/82213 [00:15<00:20, 2118.57it/s]\n",
      " 47%|                                       | 38553/82213 [00:15<00:19, 2206.80it/s]\n",
      " 47%|                                       | 38788/82213 [00:15<00:19, 2218.69it/s]\n",
      " 47%|                                      | 39035/82213 [00:15<00:18, 2284.49it/s]\n",
      " 48%|                                      | 39295/82213 [00:15<00:18, 2366.46it/s]\n",
      " 48%|                                      | 39538/82213 [00:15<00:19, 2138.96it/s]\n",
      " 48%|                                      | 39775/82213 [00:15<00:19, 2199.48it/s]\n",
      " 49%|                                      | 40096/82213 [00:16<00:17, 2425.34it/s]\n",
      " 49%|                                     | 40360/82213 [00:16<00:16, 2481.30it/s]\n",
      " 49%|                                     | 40638/82213 [00:16<00:16, 2559.61it/s]\n",
      " 50%|                                     | 40908/82213 [00:16<00:15, 2595.37it/s]\n",
      " 50%|                                     | 41173/82213 [00:16<00:15, 2576.59it/s]\n",
      " 50%|                                    | 41460/82213 [00:16<00:15, 2653.19it/s]\n",
      " 51%|                                    | 41736/82213 [00:16<00:15, 2679.57it/s]\n",
      " 51%|                                    | 42007/82213 [00:16<00:15, 2637.13it/s]\n",
      " 51%|                                    | 42294/82213 [00:16<00:15, 2639.07it/s]\n",
      " 52%|                                   | 42613/82213 [00:17<00:14, 2778.65it/s]\n",
      " 52%|                                   | 42935/82213 [00:17<00:13, 2893.02it/s]\n",
      " 53%|                                   | 43228/82213 [00:17<00:13, 2873.03it/s]\n",
      " 53%|                                  | 43518/82213 [00:17<00:14, 2701.15it/s]\n",
      " 53%|                                  | 43792/82213 [00:17<00:15, 2405.24it/s]\n",
      " 54%|                                  | 44051/82213 [00:17<00:15, 2453.37it/s]\n",
      " 54%|                                  | 44334/82213 [00:17<00:14, 2550.97it/s]\n",
      " 54%|                                 | 44595/82213 [00:17<00:16, 2221.28it/s]\n",
      " 55%|                                 | 44838/82213 [00:17<00:16, 2275.93it/s]\n",
      " 55%|                                 | 45074/82213 [00:18<00:16, 2201.13it/s]\n",
      " 55%|                                 | 45301/82213 [00:18<00:17, 2148.85it/s]\n",
      " 55%|                                 | 45537/82213 [00:18<00:16, 2204.15it/s]\n",
      " 56%|                                | 45775/82213 [00:18<00:16, 2206.80it/s]\n",
      " 56%|                                | 46030/82213 [00:18<00:15, 2295.68it/s]\n",
      " 56%|                                | 46316/82213 [00:18<00:14, 2436.13it/s]\n",
      " 57%|                                | 46564/82213 [00:18<00:19, 1875.42it/s]\n",
      " 57%|                                | 46775/82213 [00:18<00:18, 1905.65it/s]\n",
      " 57%|                               | 47024/82213 [00:18<00:17, 2046.74it/s]\n",
      " 57%|                               | 47243/82213 [00:19<00:17, 2054.96it/s]\n",
      " 58%|                               | 47470/82213 [00:19<00:16, 2111.12it/s]\n",
      " 58%|                               | 47689/82213 [00:19<00:16, 2042.16it/s]\n",
      " 58%|                              | 47931/82213 [00:19<00:16, 2138.73it/s]\n",
      " 59%|                              | 48159/82213 [00:19<00:15, 2175.42it/s]\n",
      " 59%|                              | 48399/82213 [00:19<00:15, 2234.25it/s]\n",
      " 59%|                              | 48626/82213 [00:19<00:15, 2158.61it/s]\n",
      " 59%|                              | 48845/82213 [00:19<00:15, 2108.33it/s]\n",
      " 60%|                             | 49080/82213 [00:19<00:15, 2171.59it/s]\n",
      " 60%|                             | 49300/82213 [00:20<00:15, 2169.55it/s]\n",
      " 60%|                             | 49519/82213 [00:20<00:15, 2158.82it/s]\n",
      " 61%|                             | 49760/82213 [00:20<00:14, 2224.54it/s]\n",
      " 61%|                             | 49984/82213 [00:20<00:14, 2173.79it/s]\n",
      " 61%|                            | 50216/82213 [00:20<00:14, 2211.63it/s]\n",
      " 61%|                            | 50465/82213 [00:20<00:13, 2284.34it/s]\n",
      " 62%|                            | 50705/82213 [00:20<00:13, 2313.57it/s]\n",
      " 62%|                            | 50945/82213 [00:20<00:13, 2334.47it/s]\n",
      " 62%|                            | 51180/82213 [00:20<00:13, 2280.98it/s]\n",
      " 63%|                           | 51420/82213 [00:20<00:13, 2311.14it/s]\n",
      " 63%|                           | 51652/82213 [00:21<00:13, 2236.88it/s]\n",
      " 63%|                           | 51877/82213 [00:21<00:14, 2141.98it/s]\n",
      " 63%|                           | 52093/82213 [00:21<00:14, 2076.19it/s]\n",
      " 64%|                           | 52329/82213 [00:21<00:13, 2150.13it/s]\n",
      " 64%|                          | 52589/82213 [00:21<00:13, 2264.06it/s]\n",
      " 64%|                          | 52819/82213 [00:21<00:13, 2244.19it/s]\n",
      " 65%|                          | 53107/82213 [00:21<00:12, 2399.52it/s]\n",
      " 65%|                          | 53395/82213 [00:21<00:11, 2521.51it/s]\n",
      " 65%|                         | 53652/82213 [00:21<00:11, 2494.71it/s]\n",
      " 66%|                         | 53932/82213 [00:22<00:10, 2574.70it/s]\n",
      " 66%|                         | 54193/82213 [00:22<00:10, 2580.30it/s]\n",
      " 66%|                         | 54454/82213 [00:22<00:10, 2561.68it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|                        | 54712/82213 [00:22<00:10, 2503.38it/s]\n",
      " 67%|                        | 54964/82213 [00:22<00:13, 1947.92it/s]\n",
      " 67%|                        | 55219/82213 [00:22<00:12, 2093.12it/s]\n",
      " 68%|                        | 55509/82213 [00:22<00:11, 2280.30it/s]\n",
      " 68%|                       | 55754/82213 [00:22<00:11, 2311.46it/s]\n",
      " 68%|                       | 56007/82213 [00:22<00:11, 2368.68it/s]\n",
      " 68%|                       | 56253/82213 [00:23<00:10, 2363.69it/s]\n",
      " 69%|                       | 56496/82213 [00:23<00:11, 2324.84it/s]\n",
      " 69%|                      | 56810/82213 [00:23<00:10, 2517.31it/s]\n",
      " 69%|                      | 57071/82213 [00:23<00:09, 2539.66it/s]\n",
      " 70%|                      | 57388/82213 [00:23<00:09, 2696.18it/s]\n",
      " 70%|                      | 57665/82213 [00:23<00:09, 2689.63it/s]\n",
      " 71%|                     | 57961/82213 [00:23<00:08, 2760.45it/s]\n",
      " 71%|                     | 58260/82213 [00:23<00:08, 2820.19it/s]\n",
      " 71%|                     | 58545/82213 [00:23<00:08, 2791.18it/s]\n",
      " 72%|                     | 58836/82213 [00:23<00:08, 2820.31it/s]\n",
      " 72%|                    | 59120/82213 [00:24<00:08, 2732.40it/s]\n",
      " 72%|                    | 59395/82213 [00:24<00:08, 2732.38it/s]\n",
      " 73%|                    | 59670/82213 [00:24<00:08, 2693.11it/s]\n",
      " 73%|                    | 59941/82213 [00:24<00:08, 2653.78it/s]\n",
      " 73%|                   | 60208/82213 [00:24<00:08, 2563.26it/s]\n",
      " 74%|                   | 60466/82213 [00:24<00:09, 2269.67it/s]\n",
      " 74%|                   | 60710/82213 [00:24<00:09, 2314.00it/s]\n",
      " 74%|                   | 60988/82213 [00:24<00:08, 2432.44it/s]\n",
      " 74%|                   | 61237/82213 [00:24<00:08, 2396.06it/s]\n",
      " 75%|                  | 61481/82213 [00:25<00:08, 2363.15it/s]\n",
      " 75%|                  | 61721/82213 [00:25<00:09, 2208.25it/s]\n",
      " 75%|                  | 61946/82213 [00:25<00:09, 2159.73it/s]\n",
      " 76%|                  | 62179/82213 [00:25<00:09, 2204.14it/s]\n",
      " 76%|                 | 62427/82213 [00:25<00:08, 2276.21it/s]\n",
      " 76%|                 | 62657/82213 [00:25<00:08, 2207.73it/s]\n",
      " 76%|                 | 62886/82213 [00:25<00:08, 2227.61it/s]\n",
      " 77%|                 | 63137/82213 [00:25<00:08, 2301.36it/s]\n",
      " 77%|                 | 63369/82213 [00:26<00:10, 1713.56it/s]\n",
      " 77%|                | 63632/82213 [00:26<00:09, 1910.93it/s]\n",
      " 78%|                | 63847/82213 [00:26<00:09, 1962.73it/s]\n",
      " 78%|                | 64061/82213 [00:26<00:09, 1868.89it/s]\n",
      " 78%|                | 64261/82213 [00:26<00:09, 1796.64it/s]\n",
      " 78%|                | 64465/82213 [00:26<00:09, 1860.07it/s]\n",
      " 79%|               | 64718/82213 [00:26<00:08, 2017.49it/s]\n",
      " 79%|               | 65000/82213 [00:26<00:07, 2202.49it/s]\n",
      " 79%|               | 65259/82213 [00:26<00:07, 2301.90it/s]\n",
      " 80%|               | 65499/82213 [00:26<00:07, 2306.46it/s]\n",
      " 80%|              | 65761/82213 [00:27<00:06, 2388.17it/s]\n",
      " 80%|              | 66005/82213 [00:27<00:06, 2337.73it/s]\n",
      " 81%|              | 66243/82213 [00:27<00:07, 2234.95it/s]\n",
      " 81%|              | 66480/82213 [00:27<00:06, 2269.64it/s]\n",
      " 81%|              | 66710/82213 [00:27<00:07, 2107.77it/s]\n",
      " 81%|             | 66925/82213 [00:27<00:07, 2068.05it/s]\n",
      " 82%|             | 67156/82213 [00:27<00:07, 2131.36it/s]\n",
      " 82%|             | 67372/82213 [00:27<00:07, 2098.95it/s]\n",
      " 82%|             | 67584/82213 [00:27<00:07, 2070.84it/s]\n",
      " 83%|             | 67842/82213 [00:28<00:06, 2197.55it/s]\n",
      " 83%|            | 68065/82213 [00:28<00:06, 2183.84it/s]\n",
      " 83%|            | 68327/82213 [00:28<00:06, 2294.78it/s]\n",
      " 83%|            | 68560/82213 [00:28<00:06, 2261.21it/s]\n",
      " 84%|            | 68789/82213 [00:28<00:06, 2226.34it/s]\n",
      " 84%|           | 69041/82213 [00:28<00:05, 2302.78it/s]\n",
      " 84%|           | 69286/82213 [00:28<00:05, 2340.75it/s]\n",
      " 85%|           | 69522/82213 [00:28<00:05, 2335.34it/s]\n",
      " 85%|           | 69757/82213 [00:28<00:05, 2294.57it/s]\n",
      " 85%|           | 70017/82213 [00:28<00:05, 2374.40it/s]\n",
      " 85%|          | 70256/82213 [00:29<00:05, 2274.01it/s]\n",
      " 86%|          | 70517/82213 [00:29<00:04, 2361.43it/s]\n",
      " 86%|          | 70774/82213 [00:29<00:04, 2415.82it/s]\n",
      " 86%|          | 71018/82213 [00:29<00:04, 2362.95it/s]\n",
      " 87%|         | 71307/82213 [00:29<00:04, 2495.75it/s]\n",
      " 87%|         | 71560/82213 [00:29<00:05, 1904.76it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|         | 71795/82213 [00:29<00:05, 2016.23it/s]\n",
      " 88%|         | 72090/82213 [00:29<00:04, 2224.42it/s]\n",
      " 88%|        | 72369/82213 [00:30<00:04, 2364.59it/s]\n",
      " 88%|        | 72663/82213 [00:30<00:03, 2508.16it/s]\n",
      " 89%|        | 72928/82213 [00:30<00:03, 2494.76it/s]\n",
      " 89%|        | 73216/82213 [00:30<00:03, 2594.36it/s]\n",
      " 89%|       | 73497/82213 [00:30<00:03, 2650.70it/s]\n",
      " 90%|       | 73768/82213 [00:30<00:03, 2573.56it/s]\n",
      " 90%|       | 74056/82213 [00:30<00:03, 2653.48it/s]\n",
      " 90%|       | 74327/82213 [00:30<00:02, 2665.15it/s]\n",
      " 91%|      | 74606/82213 [00:30<00:02, 2696.63it/s]\n",
      " 91%|      | 74912/82213 [00:30<00:02, 2791.16it/s]\n",
      " 91%|      | 75194/82213 [00:31<00:02, 2753.99it/s]\n",
      " 92%|      | 75493/82213 [00:31<00:02, 2815.94it/s]\n",
      " 92%|     | 75777/82213 [00:31<00:02, 2784.77it/s]\n",
      " 93%|     | 76057/82213 [00:31<00:02, 2776.07it/s]\n",
      " 93%|     | 76341/82213 [00:31<00:02, 2789.67it/s]\n",
      " 93%|     | 76621/82213 [00:31<00:02, 2567.80it/s]\n",
      " 94%|    | 76882/82213 [00:31<00:02, 2538.56it/s]\n",
      " 94%|    | 77139/82213 [00:31<00:02, 2409.01it/s]\n",
      " 94%|    | 77405/82213 [00:31<00:01, 2474.76it/s]\n",
      " 94%|    | 77684/82213 [00:31<00:01, 2557.09it/s]\n",
      " 95%|   | 77943/82213 [00:32<00:01, 2510.54it/s]\n",
      " 95%|   | 78226/82213 [00:32<00:01, 2593.77it/s]\n",
      " 95%|   | 78488/82213 [00:32<00:01, 2596.60it/s]\n",
      " 96%|   | 78770/82213 [00:32<00:01, 2655.03it/s]\n",
      " 96%|  | 79049/82213 [00:32<00:01, 2689.12it/s]\n",
      " 96%|  | 79319/82213 [00:32<00:01, 2544.81it/s]\n",
      " 97%|  | 79601/82213 [00:32<00:00, 2616.88it/s]\n",
      " 97%|  | 79877/82213 [00:32<00:01, 2056.43it/s]\n",
      " 98%| | 80184/82213 [00:33<00:00, 2279.41it/s]\n",
      " 98%| | 80473/82213 [00:33<00:00, 2429.57it/s]\n",
      " 98%| | 80734/82213 [00:33<00:00, 2476.69it/s]\n",
      " 99%| | 81013/82213 [00:33<00:00, 2558.52it/s]\n",
      " 99%|| 81279/82213 [00:33<00:00, 2546.47it/s]\n",
      " 99%|| 81579/82213 [00:33<00:00, 2662.95it/s]\n",
      "100%|| 81857/82213 [00:33<00:00, 2692.21it/s]\n",
      "100%|| 82131/82213 [00:33<00:00, 2587.76it/s]\n",
      "100%|| 82213/82213 [00:33<00:00, 2433.74it/s]\n",
      "  0%|                                                                                        | 0/20766 [00:00<?, ?it/s]\n",
      "  1%|                                                                           | 271/20766 [00:00<00:07, 2692.75it/s]\n",
      "  3%|                                                                          | 522/20766 [00:00<00:07, 2629.90it/s]\n",
      "  4%|                                                                         | 804/20766 [00:00<00:07, 2679.29it/s]\n",
      "  5%|                                                                       | 1075/20766 [00:00<00:07, 2683.31it/s]\n",
      "  6%|                                                                      | 1279/20766 [00:00<00:08, 2411.51it/s]\n",
      "  7%|                                                                     | 1524/20766 [00:00<00:07, 2418.34it/s]\n",
      "  8%|                                                                    | 1738/20766 [00:00<00:08, 2278.76it/s]\n",
      "  9%|                                                                    | 1957/20766 [00:00<00:08, 2246.95it/s]\n",
      " 10%|                                                                   | 2180/20766 [00:00<00:08, 2237.52it/s]\n",
      " 12%|                                                                  | 2410/20766 [00:01<00:08, 2251.66it/s]\n",
      " 13%|                                                                 | 2637/20766 [00:01<00:08, 2252.83it/s]\n",
      " 14%|                                                                | 2891/20766 [00:01<00:07, 2327.82it/s]\n",
      " 15%|                                                               | 3147/20766 [00:01<00:07, 2388.64it/s]\n",
      " 16%|                                                              | 3414/20766 [00:01<00:07, 2462.25it/s]\n",
      " 18%|                                                             | 3683/20766 [00:01<00:06, 2521.87it/s]\n",
      " 19%|                                                            | 3938/20766 [00:01<00:06, 2525.23it/s]\n",
      " 20%|                                                           | 4194/20766 [00:01<00:06, 2530.76it/s]\n",
      " 22%|                                                          | 4476/20766 [00:01<00:06, 2606.66it/s]\n",
      " 23%|                                                          | 4738/20766 [00:01<00:06, 2545.72it/s]\n",
      " 24%|                                                         | 4999/20766 [00:02<00:06, 2559.61it/s]\n",
      " 25%|                                                        | 5275/20766 [00:02<00:05, 2612.07it/s]\n",
      " 27%|                                                       | 5537/20766 [00:02<00:05, 2586.36it/s]\n",
      " 28%|                                                      | 5812/20766 [00:02<00:05, 2628.74it/s]\n",
      " 29%|                                                     | 6076/20766 [00:02<00:07, 1993.06it/s]\n",
      " 31%|                                                    | 6374/20766 [00:02<00:06, 2209.78it/s]\n",
      " 32%|                                                   | 6624/20766 [00:02<00:06, 2285.28it/s]\n",
      " 33%|                                                  | 6898/20766 [00:02<00:05, 2401.19it/s]\n",
      " 34%|                                                 | 7152/20766 [00:02<00:05, 2422.96it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|                                                | 7404/20766 [00:03<00:06, 2150.10it/s]\n",
      " 37%|                                               | 7639/20766 [00:03<00:05, 2202.44it/s]\n",
      " 38%|                                              | 7869/20766 [00:03<00:06, 2134.91it/s]\n",
      " 39%|                                             | 8119/20766 [00:03<00:05, 2228.90it/s]\n",
      " 40%|                                            | 8369/20766 [00:03<00:05, 2299.78it/s]\n",
      " 42%|                                           | 8634/20766 [00:03<00:05, 2390.59it/s]\n",
      " 43%|                                           | 8878/20766 [00:03<00:04, 2379.86it/s]\n",
      " 44%|                                          | 9152/20766 [00:03<00:04, 2473.26it/s]\n",
      " 45%|                                         | 9414/20766 [00:03<00:04, 2510.90it/s]\n",
      " 47%|                                        | 9668/20766 [00:04<00:04, 2375.54it/s]\n",
      " 48%|                                       | 9909/20766 [00:04<00:04, 2313.58it/s]\n",
      " 49%|                                     | 10147/20766 [00:04<00:04, 2315.31it/s]\n",
      " 50%|                                     | 10411/20766 [00:04<00:04, 2399.63it/s]\n",
      " 51%|                                    | 10694/20766 [00:04<00:04, 2510.24it/s]\n",
      " 53%|                                   | 10948/20766 [00:04<00:03, 2499.68it/s]\n",
      " 54%|                                  | 11231/20766 [00:04<00:03, 2585.84it/s]\n",
      " 55%|                                 | 11492/20766 [00:04<00:03, 2588.10it/s]\n",
      " 57%|                                | 11753/20766 [00:04<00:03, 2559.62it/s]\n",
      " 58%|                               | 12036/20766 [00:04<00:03, 2630.44it/s]\n",
      " 59%|                              | 12301/20766 [00:05<00:03, 2484.99it/s]\n",
      " 61%|                             | 12596/20766 [00:05<00:03, 2604.10it/s]\n",
      " 62%|                            | 12869/20766 [00:05<00:02, 2635.74it/s]\n",
      " 63%|                           | 13135/20766 [00:05<00:02, 2614.91it/s]\n",
      " 65%|                          | 13421/20766 [00:05<00:02, 2679.08it/s]\n",
      " 66%|                         | 13691/20766 [00:05<00:02, 2626.19it/s]\n",
      " 67%|                        | 13973/20766 [00:05<00:02, 2676.58it/s]\n",
      " 69%|                       | 14253/20766 [00:05<00:02, 2707.42it/s]\n",
      " 70%|                      | 14525/20766 [00:05<00:02, 2124.25it/s]\n",
      " 71%|                     | 14789/20766 [00:06<00:02, 2252.80it/s]\n",
      " 72%|                    | 15031/20766 [00:07<00:12, 467.71it/s]\n",
      " 74%|                   | 15311/20766 [00:07<00:08, 623.23it/s]\n",
      " 75%|                  | 15607/20766 [00:07<00:06, 816.24it/s]\n",
      " 76%|                 | 15871/20766 [00:07<00:04, 1028.82it/s]\n",
      " 78%|                | 16114/20766 [00:07<00:03, 1242.84it/s]\n",
      " 79%|               | 16356/20766 [00:08<00:03, 1406.02it/s]\n",
      " 80%|               | 16586/20766 [00:08<00:02, 1576.60it/s]\n",
      " 81%|              | 16813/20766 [00:08<00:02, 1725.31it/s]\n",
      " 82%|             | 17039/20766 [00:08<00:02, 1780.68it/s]\n",
      " 83%|            | 17255/20766 [00:08<00:02, 1738.02it/s]\n",
      " 84%|           | 17493/20766 [00:08<00:01, 1888.17it/s]\n",
      " 85%|          | 17731/20766 [00:08<00:01, 1994.74it/s]\n",
      " 87%|          | 17977/20766 [00:08<00:01, 2111.09it/s]\n",
      " 88%|         | 18214/20766 [00:08<00:01, 2178.93it/s]\n",
      " 89%|        | 18443/20766 [00:09<00:01, 2122.27it/s]\n",
      " 90%|       | 18705/20766 [00:09<00:00, 2246.81it/s]\n",
      " 91%|      | 18955/20766 [00:09<00:00, 2313.11it/s]\n",
      " 92%|     | 19195/20766 [00:09<00:00, 2334.14it/s]\n",
      " 94%|    | 19442/20766 [00:09<00:00, 2368.92it/s]\n",
      " 95%|   | 19693/20766 [00:09<00:00, 2404.93it/s]\n",
      " 96%|  | 19967/20766 [00:09<00:00, 2492.35it/s]\n",
      " 97%|  | 20230/20766 [00:09<00:00, 2527.43it/s]\n",
      " 99%| | 20501/20766 [00:09<00:00, 2574.67it/s]\n",
      "100%|| 20766/20766 [00:09<00:00, 2084.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length  82213\n",
      "train_x dimension of element  5000\n",
      "train_x (text) size RAW: 0.732808 megabytes\n",
      "train_y (labels) size RAW: 0.732808 megabytes\n",
      "test_x (text) size RAW: 0.178016 megabytes\n",
      "test_y (labels) size RAW: 0.178016 megabytes\n",
      "saved test_y\n",
      "train_x Size stays the same False\n",
      "train_y Size stays the same False\n",
      "test_x Size stays the same False\n",
      "test_y Size stays the same False\n",
      "train_xNPSize (text) size: 1644.26 megabytes\n",
      "train_yNPSize (labels) size: 0.328852 megabytes\n",
      "test_xNPSize (text) size: 415.32 megabytes\n",
      "test_yNPSize (labels) size: 0.083064 megabytes\n",
      "train_xNP length  82213\n",
      "train_xNP dimension of element  2\n",
      "train_xNP size  411065000\n",
      "saving NP arrays\n",
      "NP arrays saved\n",
      "Initializing classifier\n",
      "Searching for a  RandomForestClassifier\n",
      "False\n",
      "file not found, creating new classifier\n",
      "this is the classifierName:  RandomForestClassifier\n",
      "Training...\n",
      "train_x (text) size: 1644.26 megabytes\n",
      "train_y (labels) size: 0.328852 megabytes\n",
      "train_x (text) length: 82213\n",
      "train_y (labels) length: 82213\n",
      "[[1 0 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[0 1 0 0 0]\n",
      "finished training, classifier size: 4.8e-05 megabytes\n",
      "Predicting...\n",
      "Analysing...\n",
      "analyse_results\n",
      "hello\n",
      "Precision\t0.8545699701435038\n",
      "Recall\t0.8545699701435038\n",
      "F1-score\t0.8545699701435038\n",
      "Accuracy\t0.8545699701435038\n",
      "classiferSaveFile:  dailydialog_dailydialogRandomForestClassifier.pkl\n",
      "Saved Successfully\n",
      "Total: 475 GB\n",
      "Used: 375 GB\n",
      "Free: 99 GB\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------------ ('emoint', 'emoint', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? False\n",
      "get_train_test param:\n",
      "json  unified-dataset.jsonl\n",
      "train  emoint\n",
      "test  emoint\n",
      "there were  7102  entries that were in test and  214337 that were not in test and  0  that were in train\n",
      "test was appended  7102  times\n",
      "revised there were  5665  entries that were in test and  1437  that were in train\n",
      "single\n",
      "oof\n",
      "Detected mode: single...\n",
      "5665 1437\n",
      "Getting wordlist...\n",
      "bag size 13720\n",
      "Getting emotions\n",
      "['sadness', 'noemo', 'joy', 'fear', 'anger']\n",
      "Making arrays\n",
      "checking for save files\n",
      "emotions in make_arrays:  {'sadness': 0, 'noemo': 1, 'joy': 2, 'fear': 3, 'anger': 4}\n",
      "train raw text:  0.048456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/5665 [00:00<?, ?it/s]\n",
      "  2%|                                                                           | 110/5665 [00:00<00:05, 1093.01it/s]\n",
      "  5%|                                                                         | 288/5665 [00:00<00:04, 1234.49it/s]\n",
      "  8%|                                                                      | 477/5665 [00:00<00:03, 1375.92it/s]\n",
      " 12%|                                                                   | 678/5665 [00:00<00:03, 1517.55it/s]\n",
      " 15%|                                                                 | 855/5665 [00:00<00:03, 1582.55it/s]\n",
      " 19%|                                                             | 1072/5665 [00:00<00:02, 1719.82it/s]\n",
      " 23%|                                                          | 1324/5665 [00:00<00:02, 1898.26it/s]\n",
      " 27%|                                                       | 1540/5665 [00:00<00:02, 1966.28it/s]\n",
      " 32%|                                                   | 1812/5665 [00:00<00:01, 2141.46it/s]\n",
      " 37%|                                                | 2092/5665 [00:01<00:01, 2300.39it/s]\n",
      " 42%|                                            | 2379/5665 [00:01<00:01, 2441.85it/s]\n",
      " 48%|                                       | 2708/5665 [00:01<00:01, 2642.63it/s]\n",
      " 53%|                                    | 2983/5665 [00:01<00:01, 2510.32it/s]\n",
      " 57%|                                | 3243/5665 [00:01<00:01, 2380.85it/s]\n",
      " 62%|                             | 3501/5665 [00:01<00:00, 2432.88it/s]\n",
      " 67%|                        | 3816/5665 [00:01<00:00, 2607.08it/s]\n",
      " 73%|                    | 4135/5665 [00:01<00:00, 2492.58it/s]\n",
      " 79%|                | 4454/5665 [00:01<00:00, 2663.27it/s]\n",
      " 84%|            | 4760/5665 [00:01<00:00, 2766.24it/s]\n",
      " 89%|        | 5045/5665 [00:02<00:00, 2785.36it/s]\n",
      " 95%|   | 5390/5665 [00:02<00:00, 2951.38it/s]\n",
      "100%|| 5665/5665 [00:02<00:00, 2465.24it/s]\n",
      "  0%|                                                                                         | 0/1437 [00:00<?, ?it/s]\n",
      " 22%|                                                            | 311/1437 [00:00<00:00, 3090.21it/s]\n",
      " 38%|                                               | 549/1437 [00:00<00:00, 2829.83it/s]\n",
      " 58%|                                | 831/1437 [00:00<00:00, 2821.44it/s]\n",
      " 80%|               | 1149/1437 [00:00<00:00, 2915.08it/s]\n",
      " 97%|  | 1400/1437 [00:00<00:00, 2774.32it/s]\n",
      "100%|| 1437/1437 [00:00<00:00, 2784.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length  5665\n",
      "train_x dimension of element  5000\n",
      "train_x (text) size RAW: 0.048456 megabytes\n",
      "train_y (labels) size RAW: 0.048456 megabytes\n",
      "test_x (text) size RAW: 0.013 megabytes\n",
      "test_y (labels) size RAW: 0.013 megabytes\n",
      "saved test_y\n",
      "train_x Size stays the same False\n",
      "train_y Size stays the same False\n",
      "test_x Size stays the same False\n",
      "test_y Size stays the same False\n",
      "train_xNPSize (text) size: 113.3 megabytes\n",
      "train_yNPSize (labels) size: 0.02266 megabytes\n",
      "test_xNPSize (text) size: 28.74 megabytes\n",
      "test_yNPSize (labels) size: 0.005748 megabytes\n",
      "train_xNP length  5665\n",
      "train_xNP dimension of element  2\n",
      "train_xNP size  28325000\n",
      "saving NP arrays\n",
      "NP arrays saved\n",
      "Initializing classifier\n",
      "Searching for a  RandomForestClassifier\n",
      "False\n",
      "file not found, creating new classifier\n",
      "this is the classifierName:  RandomForestClassifier\n",
      "Training...\n",
      "train_x (text) size: 113.3 megabytes\n",
      "train_y (labels) size: 0.02266 megabytes\n",
      "train_x (text) length: 5665\n",
      "train_y (labels) length: 5665\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]]\n",
      "[4 4 4 4 4]\n",
      "finished training, classifier size: 4.8e-05 megabytes\n",
      "Predicting...\n",
      "Analysing...\n",
      "analyse_results\n",
      "hello\n",
      "Precision\t0.8517745302713987\n",
      "Recall\t0.8517745302713987\n",
      "F1-score\t0.8517745302713988\n",
      "Accuracy\t0.8517745302713987\n",
      "classiferSaveFile:  emoint_emointRandomForestClassifier.pkl\n",
      "Saved Successfully\n",
      "Total: 475 GB\n",
      "Used: 373 GB\n",
      "Free: 101 GB\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------------ ('emotion-cause', 'emotion-cause', 'paragraphs', 'paragraphs') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? False\n",
      "get_train_test param:\n",
      "json  unified-dataset.jsonl\n",
      "train  emotion-cause\n",
      "test  emotion-cause\n",
      "there were  2414  entries that were in test and  219025 that were not in test and  0  that were in train\n",
      "test was appended  2414  times\n",
      "revised there were  1954  entries that were in test and  460  that were in train\n",
      "single\n",
      "oof\n",
      "Detected mode: single...\n",
      "1954 460\n",
      "Getting wordlist...\n",
      "bag size 6541\n",
      "Getting emotions\n",
      "['sadness', 'noemo', 'disgust', 'joy', 'fear', 'surprise', 'anger']\n",
      "Making arrays\n",
      "checking for save files\n",
      "emotions in make_arrays:  {'sadness': 0, 'noemo': 1, 'disgust': 2, 'joy': 3, 'fear': 4, 'surprise': 5, 'anger': 6}\n",
      "train raw text:  0.016552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/1954 [00:00<?, ?it/s]\n",
      "  8%|                                                                      | 158/1954 [00:00<00:01, 1569.98it/s]\n",
      " 21%|                                                            | 417/1954 [00:00<00:00, 1777.97it/s]\n",
      " 33%|                                                   | 639/1954 [00:00<00:00, 1887.84it/s]\n",
      " 43%|                                            | 836/1954 [00:00<00:00, 1908.20it/s]\n",
      " 51%|                                     | 996/1954 [00:00<00:00, 1687.66it/s]\n",
      " 59%|                               | 1148/1954 [00:00<00:00, 1594.80it/s]\n",
      " 68%|                        | 1324/1954 [00:00<00:00, 1638.08it/s]\n",
      " 76%|                  | 1481/1954 [00:00<00:00, 1470.66it/s]\n",
      " 83%|            | 1626/1954 [00:01<00:00, 1245.95it/s]\n",
      " 90%|       | 1766/1954 [00:01<00:00, 1286.21it/s]\n",
      "100%|| 1945/1954 [00:01<00:00, 1402.72it/s]\n",
      "100%|| 1954/1954 [00:01<00:00, 1586.87it/s]\n",
      "  0%|                                                                                          | 0/460 [00:00<?, ?it/s]\n",
      " 47%|                                         | 214/460 [00:00<00:00, 2126.39it/s]\n",
      " 90%|        | 412/460 [00:00<00:00, 2076.06it/s]\n",
      "100%|| 460/460 [00:00<00:00, 1926.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length  1954\n",
      "train_x dimension of element  5000\n",
      "train_x (text) size RAW: 0.016552 megabytes\n",
      "train_y (labels) size RAW: 0.016552 megabytes\n",
      "test_x (text) size RAW: 0.003752 megabytes\n",
      "test_y (labels) size RAW: 0.003752 megabytes\n",
      "saved test_y\n",
      "train_x Size stays the same False\n",
      "train_y Size stays the same False\n",
      "test_x Size stays the same False\n",
      "test_y Size stays the same False\n",
      "train_xNPSize (text) size: 39.08 megabytes\n",
      "train_yNPSize (labels) size: 0.007816 megabytes\n",
      "test_xNPSize (text) size: 9.2 megabytes\n",
      "test_yNPSize (labels) size: 0.00184 megabytes\n",
      "train_xNP length  1954\n",
      "train_xNP dimension of element  2\n",
      "train_xNP size  9770000\n",
      "saving NP arrays\n",
      "NP arrays saved\n",
      "Initializing classifier\n",
      "Searching for a  RandomForestClassifier\n",
      "False\n",
      "file not found, creating new classifier\n",
      "this is the classifierName:  RandomForestClassifier\n",
      "Training...\n",
      "train_x (text) size: 39.08 megabytes\n",
      "train_y (labels) size: 0.007816 megabytes\n",
      "train_x (text) length: 1954\n",
      "train_y (labels) length: 1954\n",
      "[[1 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]]\n",
      "[3 3 3 3 3]\n",
      "finished training, classifier size: 4.8e-05 megabytes\n",
      "Predicting...\n",
      "Analysing...\n",
      "analyse_results\n",
      "hello\n",
      "Precision\t0.9260869565217391\n",
      "Recall\t0.9260869565217391\n",
      "F1-score\t0.9260869565217392\n",
      "Accuracy\t0.9260869565217391\n",
      "classiferSaveFile:  emotion-cause_emotion-causeRandomForestClassifier.pkl\n",
      "Saved Successfully\n",
      "Total: 475 GB\n",
      "Used: 373 GB\n",
      "Free: 101 GB\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------------ ('grounded_emotions', 'grounded_emotions', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? False\n",
      "get_train_test param:\n",
      "json  unified-dataset.jsonl\n",
      "train  grounded_emotions\n",
      "test  grounded_emotions\n",
      "there were  2585  entries that were in test and  218854 that were not in test and  0  that were in train\n",
      "test was appended  2585  times\n",
      "revised there were  2076  entries that were in test and  509  that were in train\n",
      "single\n",
      "oof\n",
      "Detected mode: single...\n",
      "2076 509\n",
      "Getting wordlist...\n",
      "bag size 8866\n",
      "Getting emotions\n",
      "['joy', 'noemo', 'sadness']\n",
      "Making arrays\n",
      "checking for save files\n",
      "emotions in make_arrays:  {'joy': 0, 'noemo': 1, 'sadness': 2}\n",
      "train raw text:  0.018664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/2076 [00:00<?, ?it/s]\n",
      " 12%|                                                                   | 256/2076 [00:00<00:00, 2543.71it/s]\n",
      " 23%|                                                           | 482/2076 [00:00<00:00, 2446.29it/s]\n",
      " 34%|                                                  | 708/2076 [00:00<00:00, 2382.43it/s]\n",
      " 48%|                                        | 990/2076 [00:00<00:00, 2494.50it/s]\n",
      " 64%|                           | 1333/2076 [00:00<00:00, 2712.50it/s]\n",
      " 79%|               | 1650/2076 [00:00<00:00, 2830.60it/s]\n",
      " 92%|      | 1910/2076 [00:00<00:00, 2717.58it/s]\n",
      "100%|| 2076/2076 [00:00<00:00, 2704.33it/s]\n",
      "  0%|                                                                                          | 0/509 [00:00<?, ?it/s]\n",
      " 43%|                                            | 218/509 [00:00<00:00, 2166.16it/s]\n",
      " 81%|              | 414/509 [00:00<00:00, 2095.59it/s]\n",
      "100%|| 509/509 [00:00<00:00, 2131.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length  2076\n",
      "train_x dimension of element  5000\n",
      "train_x (text) size RAW: 0.018664 megabytes\n",
      "train_y (labels) size RAW: 0.018664 megabytes\n",
      "test_x (text) size RAW: 0.004264 megabytes\n",
      "test_y (labels) size RAW: 0.004264 megabytes\n",
      "saved test_y\n",
      "train_x Size stays the same False\n",
      "train_y Size stays the same False\n",
      "test_x Size stays the same False\n",
      "test_y Size stays the same False\n",
      "train_xNPSize (text) size: 41.52 megabytes\n",
      "train_yNPSize (labels) size: 0.008304 megabytes\n",
      "test_xNPSize (text) size: 10.18 megabytes\n",
      "test_yNPSize (labels) size: 0.002036 megabytes\n",
      "train_xNP length  2076\n",
      "train_xNP dimension of element  2\n",
      "train_xNP size  10380000\n",
      "saving NP arrays\n",
      "NP arrays saved\n",
      "Initializing classifier\n",
      "Searching for a  RandomForestClassifier\n",
      "False\n",
      "file not found, creating new classifier\n",
      "this is the classifierName:  RandomForestClassifier\n",
      "Training...\n",
      "train_x (text) size: 41.52 megabytes\n",
      "train_y (labels) size: 0.008304 megabytes\n",
      "train_x (text) length: 2076\n",
      "train_y (labels) length: 2076\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "[2 0 0 0 0]\n",
      "finished training, classifier size: 4.8e-05 megabytes\n",
      "Predicting...\n",
      "Analysing...\n",
      "analyse_results\n",
      "hello\n",
      "Precision\t0.550098231827112\n",
      "Recall\t0.550098231827112\n",
      "F1-score\t0.550098231827112\n",
      "Accuracy\t0.550098231827112\n",
      "classiferSaveFile:  grounded_emotions_grounded_emotionsRandomForestClassifier.pkl\n",
      "Saved Successfully\n",
      "Total: 475 GB\n",
      "Used: 373 GB\n",
      "Free: 101 GB\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------------ ('isear', 'isear', 'descriptions', 'descriptions') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? False\n",
      "get_train_test param:\n",
      "json  unified-dataset.jsonl\n",
      "train  isear\n",
      "test  isear\n",
      "there were  7666  entries that were in test and  213773 that were not in test and  0  that were in train\n",
      "test was appended  7666  times\n",
      "revised there were  6133  entries that were in test and  1533  that were in train\n",
      "single\n",
      "oof\n",
      "Detected mode: single...\n",
      "6133 1533\n",
      "Getting wordlist...\n",
      "bag size 7901\n",
      "Getting emotions\n",
      "['noemo', 'disgust', 'shame', 'anger', 'sadness', 'guilt', 'fear', 'joy']\n",
      "Making arrays\n",
      "checking for save files\n",
      "emotions in make_arrays:  {'noemo': 0, 'disgust': 1, 'shame': 2, 'anger': 3, 'sadness': 4, 'guilt': 5, 'fear': 6, 'joy': 7}\n",
      "train raw text:  0.05456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/6133 [00:00<?, ?it/s]\n",
      "  6%|                                                                        | 340/6133 [00:00<00:01, 3378.40it/s]\n",
      " 11%|                                                                    | 653/6133 [00:00<00:01, 3293.17it/s]\n",
      " 16%|                                                                 | 960/6133 [00:00<00:01, 3216.40it/s]\n",
      " 21%|                                                            | 1280/6133 [00:00<00:01, 3205.03it/s]\n",
      " 25%|                                                         | 1520/6133 [00:00<00:01, 2844.35it/s]\n",
      " 29%|                                                      | 1776/6133 [00:00<00:01, 2746.97it/s]\n",
      " 34%|                                                  | 2071/6133 [00:00<00:01, 2753.15it/s]\n",
      " 39%|                                              | 2412/6133 [00:00<00:01, 2917.39it/s]\n",
      " 44%|                                          | 2720/6133 [00:00<00:01, 2958.65it/s]\n",
      " 49%|                                      | 3007/6133 [00:01<00:01, 2899.75it/s]\n",
      " 54%|                                  | 3339/6133 [00:01<00:00, 3008.75it/s]\n",
      " 59%|                               | 3637/6133 [00:01<00:00, 2968.04it/s]\n",
      " 65%|                           | 3962/6133 [00:01<00:00, 3041.65it/s]\n",
      " 70%|                       | 4266/6133 [00:01<00:00, 3000.10it/s]\n",
      " 75%|                   | 4588/6133 [00:01<00:00, 3057.27it/s]\n",
      " 80%|               | 4894/6133 [00:01<00:00, 2999.10it/s]\n",
      " 85%|           | 5216/6133 [00:01<00:00, 3056.28it/s]\n",
      " 90%|       | 5538/6133 [00:01<00:00, 3046.10it/s]\n",
      " 96%|   | 5865/6133 [00:01<00:00, 3104.31it/s]\n",
      "100%|| 6133/6133 [00:02<00:00, 3014.49it/s]\n",
      "  0%|                                                                                         | 0/1533 [00:00<?, ?it/s]\n",
      " 19%|                                                              | 292/1533 [00:00<00:00, 2901.42it/s]\n",
      " 40%|                                              | 609/1533 [00:00<00:00, 2971.75it/s]\n",
      " 59%|                               | 906/1533 [00:00<00:00, 2965.25it/s]\n",
      " 80%|               | 1219/1533 [00:00<00:00, 3007.52it/s]\n",
      " 98%| | 1504/1533 [00:00<00:00, 2952.60it/s]\n",
      "100%|| 1533/1533 [00:00<00:00, 2948.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length  6133\n",
      "train_x dimension of element  5000\n",
      "train_x (text) size RAW: 0.05456 megabytes\n",
      "train_y (labels) size RAW: 0.05456 megabytes\n",
      "test_x (text) size RAW: 0.013 megabytes\n",
      "test_y (labels) size RAW: 0.013 megabytes\n",
      "saved test_y\n",
      "train_x Size stays the same False\n",
      "train_y Size stays the same False\n",
      "test_x Size stays the same False\n",
      "test_y Size stays the same False\n",
      "train_xNPSize (text) size: 122.66 megabytes\n",
      "train_yNPSize (labels) size: 0.024532 megabytes\n",
      "test_xNPSize (text) size: 30.66 megabytes\n",
      "test_yNPSize (labels) size: 0.006132 megabytes\n",
      "train_xNP length  6133\n",
      "train_xNP dimension of element  2\n",
      "train_xNP size  30665000\n",
      "saving NP arrays\n",
      "NP arrays saved\n",
      "Initializing classifier\n",
      "Searching for a  RandomForestClassifier\n",
      "False\n",
      "file not found, creating new classifier\n",
      "this is the classifierName:  RandomForestClassifier\n",
      "Training...\n",
      "train_x (text) size: 122.66 megabytes\n",
      "train_y (labels) size: 0.024532 megabytes\n",
      "train_x (text) length: 6133\n",
      "train_y (labels) length: 6133\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]]\n",
      "[6 3 4 7 6]\n",
      "finished training, classifier size: 4.8e-05 megabytes\n",
      "Predicting...\n",
      "Analysing...\n",
      "analyse_results\n",
      "hello\n",
      "Precision\t0.5407697325505545\n",
      "Recall\t0.5407697325505545\n",
      "F1-score\t0.5407697325505545\n",
      "Accuracy\t0.5407697325505545\n",
      "classiferSaveFile:  isear_isearRandomForestClassifier.pkl\n",
      "Saved Successfully\n",
      "Total: 475 GB\n",
      "Used: 373 GB\n",
      "Free: 101 GB\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------------ ('ssec', 'ssec', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? False\n",
      "get_train_test param:\n",
      "json  unified-dataset.jsonl\n",
      "train  ssec\n",
      "test  ssec\n",
      "there were  4868  entries that were in test and  216571 that were not in test and  0  that were in train\n",
      "test was appended  4868  times\n",
      "revised there were  2912  entries that were in test and  1956  that were in train\n",
      "multi\n",
      "Detected mode: multi...\n",
      "2912 1956\n",
      "Getting wordlist...\n",
      "bag size 8995\n",
      "Getting emotions\n",
      "['sadness', 'joy', 'disgust', 'trust', 'fear', 'surprise', 'anger']\n",
      "Making arrays\n",
      "checking for save files\n",
      "emotions in make_arrays:  {'sadness': 0, 'joy': 1, 'disgust': 2, 'trust': 3, 'fear': 4, 'surprise': 5, 'anger': 6}\n",
      "train raw text:  0.02372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/2912 [00:00<?, ?it/s]\n",
      " 10%|                                                                     | 288/2912 [00:00<00:00, 2861.70it/s]\n",
      " 20%|                                                             | 580/2912 [00:00<00:00, 2873.51it/s]\n",
      " 28%|                                                       | 820/2912 [00:00<00:00, 2611.16it/s]\n",
      " 36%|                                                | 1051/2912 [00:00<00:00, 2507.62it/s]\n",
      " 46%|                                        | 1348/2912 [00:00<00:00, 2626.02it/s]\n",
      " 55%|                                  | 1594/2912 [00:00<00:00, 2568.75it/s]\n",
      " 66%|                         | 1921/2912 [00:00<00:00, 2740.76it/s]\n",
      " 75%|                   | 2178/2912 [00:00<00:00, 2433.94it/s]\n",
      " 83%|             | 2416/2912 [00:01<00:00, 1739.45it/s]\n",
      " 90%|       | 2614/2912 [00:01<00:00, 1554.94it/s]\n",
      "100%|| 2899/2912 [00:01<00:00, 1798.10it/s]\n",
      "100%|| 2912/2912 [00:01<00:00, 2147.96it/s]\n",
      "  0%|                                                                                         | 0/1956 [00:00<?, ?it/s]\n",
      " 16%|                                                                | 319/1956 [00:00<00:00, 3169.72it/s]\n",
      " 29%|                                                      | 573/1956 [00:00<00:00, 2943.70it/s]\n",
      " 41%|                                             | 804/1956 [00:00<00:00, 2713.75it/s]\n",
      " 54%|                                   | 1049/1956 [00:00<00:00, 2623.43it/s]\n",
      " 63%|                           | 1240/1956 [00:00<00:00, 1670.93it/s]\n",
      " 72%|                     | 1401/1956 [00:00<00:00, 1490.75it/s]\n",
      " 80%|               | 1557/1956 [00:00<00:00, 1508.07it/s]\n",
      " 88%|         | 1726/1956 [00:00<00:00, 1555.64it/s]\n",
      " 99%|| 1945/1956 [00:01<00:00, 1701.15it/s]\n",
      "100%|| 1956/1956 [00:01<00:00, 1845.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length  2912\n",
      "train_x dimension of element  5000\n",
      "train_x (text) size RAW: 0.02372 megabytes\n",
      "train_y (labels) size RAW: 0.02372 megabytes\n",
      "test_x (text) size RAW: 0.016552 megabytes\n",
      "test_y (labels) size RAW: 0.016552 megabytes\n",
      "saved test_y\n",
      "train_x Size stays the same False\n",
      "train_y Size stays the same False\n",
      "test_x Size stays the same False\n",
      "test_y Size stays the same False\n",
      "train_xNPSize (text) size: 58.24 megabytes\n",
      "train_yNPSize (labels) size: 0.081536 megabytes\n",
      "test_xNPSize (text) size: 39.12 megabytes\n",
      "test_yNPSize (labels) size: 0.054768 megabytes\n",
      "train_xNP length  2912\n",
      "train_xNP dimension of element  2\n",
      "train_xNP size  14560000\n",
      "saving NP arrays\n",
      "NP arrays saved\n",
      "Initializing classifier\n",
      "Searching for a  RandomForestClassifier\n",
      "False\n",
      "file not found, creating new classifier\n",
      "this is the classifierName:  RandomForestClassifier\n",
      "Training...\n",
      "train_x (text) size: 58.24 megabytes\n",
      "train_y (labels) size: 0.081536 megabytes\n",
      "train_x (text) length: 2912\n",
      "train_y (labels) length: 2912\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]]\n",
      "[[1 0 1 1 1 1 1]\n",
      " [0 1 0 1 1 0 0]\n",
      " [1 1 1 1 0 0 1]\n",
      " [1 0 1 1 1 0 1]\n",
      " [0 1 0 0 0 0 0]]\n",
      "finished training, classifier size: 4.8e-05 megabytes\n",
      "Predicting...\n",
      "Analysing...\n",
      "analyse_results\n",
      "hello\n",
      "Precision\t0.708385170440408\n",
      "Recall\t0.4375288151221761\n",
      "F1-score\t0.5409462283868516\n",
      "Accuracy\t0.09151329243353783\n",
      "classiferSaveFile:  ssec_ssecRandomForestClassifier.pkl\n",
      "Saved Successfully\n",
      "Total: 475 GB\n",
      "Used: 374 GB\n",
      "Free: 100 GB\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------------ ('tales-emotion', 'tales-emotion', 'tales', 'tales') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? False\n",
      "get_train_test param:\n",
      "json  unified-dataset.jsonl\n",
      "train  tales-emotion\n",
      "test  tales-emotion\n",
      "there were  14771  entries that were in test and  206668 that were not in test and  0  that were in train\n",
      "test was appended  14771  times\n",
      "revised there were  11773  entries that were in test and  2998  that were in train\n",
      "single\n",
      "oof\n",
      "Detected mode: single...\n",
      "11773 2998\n",
      "Getting wordlist...\n",
      "bag size 9776\n",
      "Getting emotions\n",
      "['noemo', 'disgust', 'anger', 'surprise', 'sadness', 'fear', 'joy']\n",
      "Making arrays\n",
      "checking for save files\n",
      "emotions in make_arrays:  {'noemo': 0, 'disgust': 1, 'anger': 2, 'surprise': 3, 'sadness': 4, 'fear': 5, 'joy': 6}\n",
      "train raw text:  0.098616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/11773 [00:00<?, ?it/s]\n",
      "  3%|                                                                          | 309/11773 [00:00<00:03, 3069.43it/s]\n",
      "  6%|                                                                       | 648/11773 [00:00<00:03, 3153.68it/s]\n",
      "  8%|                                                                     | 957/11773 [00:00<00:03, 3128.20it/s]\n",
      " 11%|                                                                  | 1283/11773 [00:00<00:03, 3160.71it/s]\n",
      " 13%|                                                                 | 1526/11773 [00:00<00:03, 2842.48it/s]\n",
      " 15%|                                                               | 1767/11773 [00:00<00:03, 2691.49it/s]\n",
      " 17%|                                                              | 2007/11773 [00:00<00:03, 2488.11it/s]\n",
      " 19%|                                                            | 2270/11773 [00:00<00:03, 2524.38it/s]\n",
      " 22%|                                                          | 2588/11773 [00:00<00:03, 2686.26it/s]\n",
      " 24%|                                                        | 2884/11773 [00:01<00:03, 2757.95it/s]\n",
      " 27%|                                                      | 3207/11773 [00:01<00:02, 2879.48it/s]\n",
      " 30%|                                                    | 3494/11773 [00:01<00:02, 2862.87it/s]\n",
      " 32%|                                                  | 3815/11773 [00:01<00:02, 2953.41it/s]\n",
      " 35%|                                                | 4111/11773 [00:01<00:02, 2924.10it/s]\n",
      " 37%|                                               | 4404/11773 [00:01<00:02, 2877.59it/s]\n",
      " 40%|                                             | 4714/11773 [00:01<00:02, 2935.77it/s]\n",
      " 43%|                                           | 5009/11773 [00:01<00:02, 2883.24it/s]\n",
      " 45%|                                         | 5315/11773 [00:01<00:02, 2928.70it/s]\n",
      " 48%|                                       | 5614/11773 [00:01<00:02, 2941.26it/s]\n",
      " 50%|                                     | 5934/11773 [00:02<00:01, 3008.90it/s]\n",
      " 53%|                                   | 6236/11773 [00:02<00:02, 2480.44it/s]\n",
      " 56%|                                 | 6556/11773 [00:02<00:01, 2655.45it/s]\n",
      " 58%|                               | 6869/11773 [00:02<00:01, 2777.45it/s]\n",
      " 61%|                             | 7159/11773 [00:02<00:01, 2768.20it/s]\n",
      " 63%|                           | 7453/11773 [00:02<00:01, 2812.22it/s]\n",
      " 66%|                         | 7741/11773 [00:02<00:01, 2747.26it/s]\n",
      " 68%|                       | 8049/11773 [00:02<00:01, 2834.28it/s]\n",
      " 71%|                      | 8337/11773 [00:02<00:01, 2738.45it/s]\n",
      " 73%|                    | 8615/11773 [00:03<00:01, 2551.62it/s]\n",
      " 76%|                  | 8890/11773 [00:03<00:01, 2603.31it/s]\n",
      " 78%|                | 9179/11773 [00:03<00:00, 2678.20it/s]\n",
      " 81%|              | 9501/11773 [00:03<00:00, 2815.82it/s]\n",
      " 83%|            | 9787/11773 [00:03<00:00, 2767.20it/s]\n",
      " 86%|          | 10067/11773 [00:03<00:00, 1949.24it/s]\n",
      " 87%|         | 10298/11773 [00:03<00:00, 1788.88it/s]\n",
      " 89%|        | 10505/11773 [00:04<00:00, 1698.83it/s]\n",
      " 92%|      | 10782/11773 [00:04<00:00, 1919.10it/s]\n",
      " 94%|    | 11061/11773 [00:04<00:00, 2114.45it/s]\n",
      " 96%|  | 11346/11773 [00:04<00:00, 2288.37it/s]\n",
      " 99%|| 11651/11773 [00:04<00:00, 2469.83it/s]\n",
      "100%|| 11773/11773 [00:04<00:00, 2630.53it/s]\n",
      "  0%|                                                                                         | 0/2998 [00:00<?, ?it/s]\n",
      " 10%|                                                                     | 313/2998 [00:00<00:00, 3110.15it/s]\n",
      " 20%|                                                             | 592/2998 [00:00<00:00, 3000.17it/s]\n",
      " 30%|                                                      | 892/2998 [00:00<00:00, 2994.34it/s]\n",
      " 40%|                                             | 1191/2998 [00:00<00:00, 2987.57it/s]\n",
      " 48%|                                       | 1450/2998 [00:00<00:00, 2849.76it/s]\n",
      " 57%|                                | 1709/2998 [00:00<00:00, 2761.09it/s]\n",
      " 66%|                         | 1990/2998 [00:00<00:00, 2770.33it/s]\n",
      " 75%|                   | 2246/2998 [00:00<00:00, 2698.21it/s]\n",
      " 85%|           | 2548/2998 [00:00<00:00, 2782.18it/s]\n",
      " 94%|    | 2816/2998 [00:01<00:00, 2136.53it/s]\n",
      "100%|| 2998/2998 [00:01<00:00, 2564.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length  11773\n",
      "train_x dimension of element  5000\n",
      "train_x (text) size RAW: 0.098616 megabytes\n",
      "train_y (labels) size RAW: 0.098616 megabytes\n",
      "test_x (text) size RAW: 0.026728 megabytes\n",
      "test_y (labels) size RAW: 0.026728 megabytes\n",
      "saved test_y\n",
      "train_x Size stays the same False\n",
      "train_y Size stays the same False\n",
      "test_x Size stays the same False\n",
      "test_y Size stays the same False\n",
      "train_xNPSize (text) size: 235.46 megabytes\n",
      "train_yNPSize (labels) size: 0.047092 megabytes\n",
      "test_xNPSize (text) size: 59.96 megabytes\n",
      "test_yNPSize (labels) size: 0.011992 megabytes\n",
      "train_xNP length  11773\n",
      "train_xNP dimension of element  2\n",
      "train_xNP size  58865000\n",
      "saving NP arrays\n",
      "NP arrays saved\n",
      "Initializing classifier\n",
      "Searching for a  RandomForestClassifier\n",
      "False\n",
      "file not found, creating new classifier\n",
      "this is the classifierName:  RandomForestClassifier\n",
      "Training...\n",
      "train_x (text) size: 235.46 megabytes\n",
      "train_y (labels) size: 0.047092 megabytes\n",
      "train_x (text) length: 11773\n",
      "train_y (labels) length: 11773\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n",
      "[0 0 0 0 0]\n",
      "finished training, classifier size: 4.8e-05 megabytes\n",
      "Predicting...\n",
      "Analysing...\n",
      "analyse_results\n",
      "hello\n",
      "Precision\t0.6564376250833889\n",
      "Recall\t0.6564376250833889\n",
      "F1-score\t0.6564376250833889\n",
      "Accuracy\t0.6564376250833889\n",
      "classiferSaveFile:  tales-emotion_tales-emotionRandomForestClassifier.pkl\n",
      "Saved Successfully\n",
      "Total: 475 GB\n",
      "Used: 374 GB\n",
      "Free: 100 GB\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------------ ('tec', 'tec', 'tweets', 'tweets') -------------------------------------------\n",
      "Getting data\n",
      "do pickle files exist? False\n",
      "get_train_test param:\n",
      "json  unified-dataset.jsonl\n",
      "train  tec\n",
      "test  tec\n",
      "there were  21051  entries that were in test and  200388 that were not in test and  0  that were in train\n",
      "test was appended  21051  times\n",
      "revised there were  16819  entries that were in test and  4232  that were in train\n",
      "single\n",
      "oof\n",
      "Detected mode: single...\n",
      "16819 4232\n",
      "Getting wordlist...\n",
      "bag size 25583\n",
      "Getting emotions\n",
      "['sadness', 'noemo', 'disgust', 'joy', 'fear', 'surprise', 'anger']\n",
      "Making arrays\n",
      "checking for save files\n",
      "emotions in make_arrays:  {'sadness': 0, 'noemo': 1, 'disgust': 2, 'joy': 3, 'fear': 4, 'surprise': 5, 'anger': 6}\n",
      "train raw text:  0.140576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/16819 [00:00<?, ?it/s]\n",
      "  2%|                                                                          | 326/16819 [00:00<00:05, 3239.27it/s]\n",
      "  4%|                                                                         | 625/16819 [00:00<00:05, 3153.84it/s]\n",
      "  5%|                                                                        | 894/16819 [00:00<00:05, 2982.49it/s]\n",
      "  7%|                                                                      | 1148/16819 [00:00<00:05, 2828.31it/s]\n",
      "  8%|                                                                     | 1357/16819 [00:00<00:06, 2436.83it/s]\n",
      "  9%|                                                                    | 1559/16819 [00:00<00:06, 2289.78it/s]\n",
      " 11%|                                                                  | 1874/16819 [00:00<00:06, 2490.33it/s]\n",
      " 13%|                                                                 | 2137/16819 [00:00<00:05, 2525.96it/s]\n",
      " 14%|                                                                | 2380/16819 [00:00<00:05, 2432.43it/s]\n",
      " 16%|                                                               | 2630/16819 [00:01<00:05, 2447.70it/s]\n",
      " 17%|                                                              | 2921/16819 [00:01<00:05, 2565.66it/s]\n",
      " 19%|                                                            | 3253/16819 [00:01<00:04, 2748.94it/s]\n",
      " 21%|                                                           | 3531/16819 [00:01<00:04, 2753.19it/s]\n",
      " 23%|                                                         | 3856/16819 [00:01<00:04, 2880.40it/s]\n",
      " 25%|                                                        | 4147/16819 [00:01<00:04, 2770.18it/s]\n",
      " 26%|                                                       | 4427/16819 [00:01<00:04, 2535.82it/s]\n",
      " 28%|                                                      | 4687/16819 [00:01<00:04, 2436.75it/s]\n",
      " 29%|                                                     | 4936/16819 [00:01<00:04, 2378.64it/s]\n",
      " 31%|                                                    | 5178/16819 [00:02<00:05, 2292.68it/s]\n",
      " 32%|                                                  | 5411/16819 [00:02<00:05, 2272.84it/s]\n",
      " 34%|                                                 | 5641/16819 [00:02<00:05, 1931.25it/s]\n",
      " 35%|                                                | 5909/16819 [00:02<00:05, 2104.90it/s]\n",
      " 37%|                                               | 6185/16819 [00:02<00:04, 2262.71it/s]\n",
      " 39%|                                              | 6480/16819 [00:02<00:04, 2428.90it/s]\n",
      " 40%|                                            | 6762/16819 [00:02<00:03, 2529.98it/s]\n",
      " 42%|                                           | 7039/16819 [00:02<00:03, 2592.83it/s]\n",
      " 44%|                                          | 7365/16819 [00:02<00:03, 2757.94it/s]\n",
      " 45%|                                         | 7649/16819 [00:03<00:03, 2683.43it/s]\n",
      " 47%|                                       | 7956/16819 [00:03<00:03, 2784.10it/s]\n",
      " 49%|                                      | 8271/16819 [00:03<00:02, 2879.36it/s]\n",
      " 51%|                                    | 8564/16819 [00:03<00:02, 2872.12it/s]\n",
      " 53%|                                   | 8855/16819 [00:03<00:02, 2812.07it/s]\n",
      " 54%|                                  | 9139/16819 [00:03<00:02, 2758.37it/s]\n",
      " 56%|                                 | 9441/16819 [00:03<00:02, 2826.89it/s]\n",
      " 58%|                               | 9726/16819 [00:03<00:02, 2828.38it/s]\n",
      " 60%|                              | 10025/16819 [00:03<00:02, 2869.93it/s]\n",
      " 61%|                            | 10320/16819 [00:03<00:02, 2887.79it/s]\n",
      " 63%|                           | 10610/16819 [00:04<00:02, 2877.45it/s]\n",
      " 65%|                          | 10899/16819 [00:04<00:02, 2859.11it/s]\n",
      " 67%|                        | 11186/16819 [00:04<00:01, 2848.51it/s]\n",
      " 68%|                       | 11496/16819 [00:04<00:01, 2914.08it/s]\n",
      " 70%|                      | 11788/16819 [00:04<00:01, 2802.99it/s]\n",
      " 72%|                     | 12070/16819 [00:04<00:01, 2746.16it/s]\n",
      " 74%|                   | 12377/16819 [00:04<00:01, 2830.67it/s]\n",
      " 75%|                  | 12662/16819 [00:04<00:01, 2798.35it/s]\n",
      " 77%|                 | 12967/16819 [00:04<00:01, 2864.20it/s]\n",
      " 79%|               | 13256/16819 [00:04<00:01, 2866.43it/s]\n",
      " 81%|              | 13544/16819 [00:05<00:01, 2856.59it/s]\n",
      " 82%|             | 13831/16819 [00:05<00:01, 2830.14it/s]\n",
      " 84%|            | 14115/16819 [00:05<00:01, 2149.75it/s]\n",
      " 86%|          | 14405/16819 [00:05<00:01, 2327.05it/s]\n",
      " 87%|         | 14660/16819 [00:05<00:01, 1323.04it/s]\n",
      " 89%|        | 14961/16819 [00:05<00:01, 1588.83it/s]\n",
      " 91%|       | 15235/16819 [00:06<00:00, 1815.66it/s]\n",
      " 92%|     | 15493/16819 [00:06<00:00, 1989.82it/s]\n",
      " 94%|    | 15764/16819 [00:06<00:00, 2158.90it/s]\n",
      " 95%|   | 16024/16819 [00:06<00:00, 2270.85it/s]\n",
      " 97%|  | 16279/16819 [00:06<00:00, 2299.85it/s]\n",
      " 98%| | 16542/16819 [00:06<00:00, 2385.71it/s]\n",
      "100%|| 16813/16819 [00:06<00:00, 2470.37it/s]\n",
      "100%|| 16819/16819 [00:06<00:00, 2520.89it/s]\n",
      "  0%|                                                                                         | 0/4232 [00:00<?, ?it/s]\n",
      "  7%|                                                                       | 301/4232 [00:00<00:01, 2990.90it/s]\n",
      " 13%|                                                                  | 567/4232 [00:00<00:01, 2877.05it/s]\n",
      " 21%|                                                             | 878/4232 [00:00<00:01, 2938.07it/s]\n",
      " 27%|                                                       | 1158/4232 [00:00<00:01, 2889.27it/s]\n",
      " 34%|                                                  | 1454/4232 [00:00<00:00, 2904.87it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|                                            | 1745/4232 [00:00<00:00, 2900.86it/s]\n",
      " 48%|                                       | 2014/4232 [00:00<00:00, 2828.48it/s]\n",
      " 54%|                                   | 2276/4232 [00:00<00:00, 2756.72it/s]\n",
      " 60%|                              | 2540/4232 [00:00<00:00, 2715.27it/s]\n",
      " 68%|                        | 2863/4232 [00:01<00:00, 2846.77it/s]\n",
      " 75%|                   | 3172/4232 [00:01<00:00, 2910.56it/s]\n",
      " 82%|             | 3460/4232 [00:01<00:00, 2869.88it/s]\n",
      " 89%|        | 3772/4232 [00:01<00:00, 2935.29it/s]\n",
      " 96%|   | 4065/4232 [00:01<00:00, 2852.37it/s]\n",
      "100%|| 4232/4232 [00:01<00:00, 2867.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length  16819\n",
      "train_x dimension of element  5000\n",
      "train_x (text) size RAW: 0.140576 megabytes\n",
      "train_y (labels) size RAW: 0.140576 megabytes\n",
      "test_x (text) size RAW: 0.03392 megabytes\n",
      "test_y (labels) size RAW: 0.03392 megabytes\n",
      "saved test_y\n",
      "train_x Size stays the same False\n",
      "train_y Size stays the same False\n",
      "test_x Size stays the same False\n",
      "test_y Size stays the same False\n",
      "train_xNPSize (text) size: 336.38 megabytes\n",
      "train_yNPSize (labels) size: 0.067276 megabytes\n",
      "test_xNPSize (text) size: 84.64 megabytes\n",
      "test_yNPSize (labels) size: 0.016928 megabytes\n",
      "train_xNP length  16819\n",
      "train_xNP dimension of element  2\n",
      "train_xNP size  84095000\n",
      "saving NP arrays\n",
      "NP arrays saved\n",
      "Initializing classifier\n",
      "Searching for a  RandomForestClassifier\n",
      "False\n",
      "file not found, creating new classifier\n",
      "this is the classifierName:  RandomForestClassifier\n",
      "Training...\n",
      "train_x (text) size: 336.38 megabytes\n",
      "train_y (labels) size: 0.067276 megabytes\n",
      "train_x (text) length: 16819\n",
      "train_y (labels) length: 16819\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]]\n",
      "[5 0 3 3 2]\n",
      "finished training, classifier size: 4.8e-05 megabytes\n",
      "Predicting...\n",
      "Analysing...\n",
      "analyse_results\n",
      "hello\n",
      "Precision\t0.5420604914933838\n",
      "Recall\t0.5420604914933838\n",
      "F1-score\t0.5420604914933838\n",
      "Accuracy\t0.5420604914933838\n",
      "classiferSaveFile:  tec_tecRandomForestClassifier.pkl\n",
      "Saved Successfully\n",
      "Total: 475 GB\n",
      "Used: 374 GB\n",
      "Free: 100 GB\n",
      "-----------------------------------------------------------------------------------------\n",
      "------------------------------ (None, 'affectivetext', None, 'headlines') -------------------------------------------\n",
      "Getting data\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-379-6f913454dede>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mallVs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mverifyResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mrunTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifyResults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrossCorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msameCorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallVs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-378-a849aa413cc6>\u001b[0m in \u001b[0;36mrunTrials\u001b[1;34m(version, verifyResults, crossCorpus, sameCorpus, allVs)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0msortedPermutations\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgetAllVsCorpusValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibleChoices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcorpusPair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msortedPermutations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mperformTrialUsingCorpusPair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpusPair\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifyResults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#version == \"myTrials\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mpowerSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetPowerset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibleChoices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-377-4d0ad757aaee>\u001b[0m in \u001b[0;36mperformTrialUsingCorpusPair\u001b[1;34m(corpusPair, verifyResults)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverifyResults\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtrain_xNPFileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msecond\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"train_xNP\"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtrain_yNPFileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msecond\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"train_yNP\"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtest_xNPFileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msecond\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"test_xNP\"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    version = \"previous\"\n",
    "#     version = \"myTrials\"\n",
    "    crossCorpus = True\n",
    "    sameCorpus = False\n",
    "    allVs = False\n",
    "    verifyResults = False\n",
    "    runTrials(version, verifyResults, crossCorpus, sameCorpus, allVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
